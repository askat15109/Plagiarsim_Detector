GITA: Graph to Visual and Textual Integration
for Vision-Language Graph Reasoning
Yanbin Wei1,2∗, Shuai Fu1,3∗, Weisen Jiang1,2†, Zejian Zhang4, Zhixiong Zeng4,
Qi Wu3, James T. Kwok2, Yu Zhang1†
1Department of Computer Science and Engineering, Southern University of Science and Technology
2Department of Computer Science and Engineering, Hong Kong University of Science and Technology
3Australia Institute for Machine Learning, University of Adelaide
4Tencent
{yanbin.ust, fus.jayce, zejianzhang33, yu.zhang.ust, waysonkong}@gmail.com
barretzeng@tencent.com, qi.wu01@adelaide.edu.au, jamesk@cse.ust.hk
Abstract
Large Language Models (LLMs) are increasingly used for various tasks with
graph structures. Though LLMs can process graph information in the textual
format, they overlook the rich vision modality, which is an intuitive way for
humans to comprehend structural information and conduct general graph reasoning.
The potential benefits and capabilities of representing graph structures as visual
images (i.e., visual graph) are still unexplored. To fill the gap, we innovatively
propose an end-to-end framework, called Graph to vIsual and Textual IntegrAtion
(GITA), which incorporates visual graphs into general graph reasoning. Besides,
we construct the Graph-based Vision-Language Question Answering (GVLQA)
dataset from existing graph data, which is the first vision-language dataset for
general graph reasoning. Extensive experiments on the GVLQA dataset and five
real-world datasets show that GITA outperforms mainstream LLMs on general
graph reasoning. Moreover, experimental results demonstrate the effectiveness of
the layout augmentation on visual graphs and pretraining on the GVLQA dataset.
1
Introduction
Graph reasoning tasks are pivotal in domains such as recommendation systems [25, 60], social
network analysis [7, 29], and knowledge graph reasoning [72, 46, 62]. Various architectures have
been developed, from shallow embedding methods [6, 53] to advanced Graph Neural Networks
(GNNs) [37, 64] and graph Transformers [71, 40, 8]. While these models excel in graph reasoning
tasks, they often lack generalizability, flexibility, and user-friendliness. Achieving good performance
with these models typically requires domain-specific tuning, which limits their abilities to generalize
across different domains. Additionally, these models struggle to handle diverse tasks with the same
architecture. Each task often requires a specialized design, including task-specific data processing
and decoder, leading to limited flexibility. Lastly, unlike the Large Language Models (LLMs) that
can engage in conversations with users, these models are less explainable and user-friendly.
In contrast, LLMs have shown great generalization capabilities across a wide variety of reasoning
tasks [61, 67, 74, 32, 33] by encapsulating task-specific demands within a cohesive and interpretable
mechanism - text prompts, under a unified architecture with minimal domain-specific adjustments.
These advantages have sparked investigations into the potential of LLMs for graph reasoning. Recent
developments lend credence to the notion that LLMs can indeed interpret and manipulate graph-
∗Equal contribution.
†Corresponding authors.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).

structured data through textual representations. For example, InstructGLM [66], GPT4Graph [21],
and LLMtoGraph [44] convert graphs into textual descriptions and then use these descriptions paired
with queries to enable LLMs to generate accurate responses for graph reasoning tasks. Furthermore,
the introduction of benchmarks such as GraphQA [14] and NLGraph [59] is a testament to the
growing interest in evaluating LLMs’ effectiveness on graph reasoning tasks framed in natural
languages.
Despite the development of numerous methods and benchmarks for graph reasoning on LLMs, they
often overlook the valuable vision modality, which is a natural means for humans to comprehend
structural information and has demonstrated its success in various visual reasoning scenarios [30, 68,
69, 34, 3, 54]. Consequently, the following questions arise: (1) Can incorporating visual information
be beneficial for general graph reasoning scenarios? (2) If so, how can we effectively integrate
the vision modality into graph reasoning? To the best of our knowledge, these questions remain
unexplored.
To answer them, we first propose an end-to-end framework called Graph to vIsual and Textual
IntegrAtion (GITA)34 that systematically integrates visual information into instruction-based graph
reasoning, by rendering graph structures to customized visual images which are called visual graph.
Specifically, the GITA framework has four components: a graph visualizer for generating visual
graphs, a graph describer for producing textual descriptions of the graph structure, a task-based
questioner that organizes the description and requirements of the current task into prompt instruction,
and a Vision-Language Model (VLM) to perform vision-language graph reasoning. In the proposed
GITA framework, visual information can be incorporated into many tasks with explicit or implicit
graph structures, without sacrificing its versatility, flexibility, or user-friendliness. Besides, since
there is no dataset for vision-supported general graph reasoning capabilities, we construct the first
vision-language dataset for general graph reasoning purposes called Graph-based Vision-Language
Question Answering (GVLQA)5 based on the proposed GITA framework. The GVLQA dataset
consists of 526K instances covering seven representative graph reasoning tasks, aiming to thoroughly
evaluate the structure-based graph reasoning abilities of VLMs and LLMs. Extensive experiments on
the GVLQA dataset and five real-world datasets demonstrate the effectiveness of the proposed GITA
model. Furthermore, we delve into the effects of visual graph augmentation strategies and find that
layout augmentation can dramatically boost vision-based graph reasoning performance.
Our main contributions are summarized as follows.
• We introduce an end-to-end GITA framework, innovatively integrating vision modality to
boost the graph reasoning abilities of language models.
• We establish GVLQA, the first vision-language question-answering dataset for general
graph reasoning purposes. It can be used to thoroughly evaluate the structure-based graph
reasoning abilities of LLMs/VLMs and can also be used as pretraining data to boost the
performance of downstream tasks.
• Extensive experiments on benchmark datasets across various graph reasoning tasks demon-
strate the effectiveness of the proposed GITA framework and the benefits of layout augmen-
tation on visual graphs.
2
Related Work
Graph Reasoning. Graph reasoning [5, 63] aims to answer questions based on graphs, which involves
utilizing graph structures to guide the reasoning process to generate answers. Graph reasoning has a
wide variety of applications in social network analysis [47, 41], bioinformatics [31, 18], chemistry
[19], physics [4], knowledge graph reasoning [6], and recommendation systems [39, 26]. Many graph
reasoning methods have been proposed. Early attempts [6, 53] learn node and edge representations
through shallow modules, which may have only limited expressive power. Graph Neural Networks
(GNNs) such as GCN [37], GAT [58], GraphSAGE [23], MPNN [19], and GIN [64] use message-
passing paradigm [19] to model graph dependencies and update node features. Transformer-based
3Project Homepage: v-graph.github.io.
4Code Repository: https://github.com/WEIYanbin1999/GITA/.
5Dataset: https://huggingface.co/collections/Yanbin99/.
2

graph models [71, 40, 8] further propose to use self-attention to increase the expressiveness and long-
range dependency. However, as discussed in Sec 1, these models may exhibit limited generalizability,
flexibility, and user-friendliness.
LLMs on Graph Reasoning. There have been many attempts to use LLMs in graph reasoning.
Depending on how they align the input spaces of graphs and LLMs, we categorize them into two
types: Graph-to-text and Graph-to-token. Graph-to-text methods transform a graph into textual
descriptions, which are concatenated with the instructions and fed to the LLM for querying. For
example, InstructGLM [66] uses natural language to describe the graph and proposes instruction
prompts to fine-tune the LLM. He et.al [27] applies LLMs to explain graphs for training GNNs,
while Chen et.al [10] treat LLMs as enhancers to exploit text attributes or as predictors for node
classification on text-attributed graphs. GPT4Graph [21] and LLMtoGraph [44] convert graphs
into specific code or natural language formats by the powerful ChatGPT [48, 49]. On the other
hand, Graph-to-token methods include GraphGPT [55], GraphToken [50] and LLaGA [9]. For these
methods, the graph is represented as a specially designed token sequence, which is projected or
merged into the LLM’s token space for text-based reasoning. However, none of the aforementioned
methods represent the graph structure information as images, highlighting the uniqueness of the
proposed GITA framework and GVLQA dataset.
Large Vision-Language Models. Large VLMs have significantly expanded the cognitive abilities
of LLMs by integrating the vision modality to address vision-language tasks. Many methods have
been proposed. Some early explorations like Flamingo [2], CLIP [51], and BLIP-2 [43] use a visual
encoder for processing images and align the visual and textual embeddings. Subsequent models like
LLaVA [45] and MiniGPT-4 [75] combine visual and textual inputs in a single LLM for solving
multimodal tasks. InstructBlip [11] proposes an instruction-aware query transformer and trains a
vision-language model by instruction tuning. However, despite progress in a wide range of vision-
language tasks [70, 16], using visual information in graph reasoning remains overlooked. We take
the first step in this field, pushing the boundaries of VLMs in graph reasoning.
3
GITA: Graph to Visual and Textual Integration
3.1
Preliminary
Graph Reasoning. In traditional graph reasoning settings, models typically rely on two main inputs:
(i) the graph structure G = {C, E}, where C and E are the set of vertices and edges, respectively;
(ii) the task requirement T, encompassing specific operations or questions pertaining to the graph.
Based on the information provided in G and a specific task requirement T, models are expected to
output a reasonable answer A. On the other hand, in the context of instruction-based graph reasoning
methods, it is necessary to convert these inputs into textual form. This transformation facilities
graph reasoning within natural language, allowing for improved interpretation and harnessing the
formidable reasoning capabilities of large language models.
3.2
Architecture
Overview. Different from the above graph reasoning methods, we propose a Graph to Image-Txt
Assistant (GITA), which is the first attempt to perform graph reasoning in a vision-text-based manner.
GITA comprises four pivotal components: a task-agnostic graph visualizer V , a graph describer D,
a task-specific questioner Q, and a VLM reasoner Rϕ, as illustrated in Figure 1. Firstly, V and G
are designed to produce visual depictions (i.e., visual graphs) and textual descriptions of the graph
structure inputs, respectively. Then, given the task requirement T and the textual description produced
by D, Q is designed to form a task-specific query. Finally, Rϕ receives the visual input IG from
V based on the visual graph and the textual input QT
G from Q, then generates answers A in natural
language. In the following, we introduce the four components in detail.
Graph Visualizer. The role of the graph visualizer is to generate visual graphs from structural
graphs. The image representation of a structural graph is not unique, as there can be variations in
many aspects, such as backdrop colors, layouts, and node shapes. These variations may enhance the
robustness of models through effective training but simultaneously increase the learning difficulty
for models. Therefore, balancing consistency and variety is necessary during the graph visualization
process. This trade-off is reflected in our design of graph visualizer, by maintaining consistency in
3

GITA Framewrok
Graph
Data
Graph Visualizer
Configurations
Base Image Styles
Graph-related
Image Styles
Textual Description
In a directed graph,
(i,j) means that node i and
node j are connected with
an undirected edge.
The nodes are  numbered
from 0 to 6, and the edges
are:
(0,2) (2,6) ...
Graph Describer
Task-agnostic Graph
Describing
Templates
Visual Graph
2
6
1
3
5
4
0
OR
OR ...
(w/ diff
structure-
aware configs)
2
6
1
3
5
4
0
Input tokens
Graph Descriptions
...
The nodes are 
numbered from
0 to 6, and the
edges are:
(0,2)
(2,6)
(1,4)
...
Traditional LLM Solutions
Graph
Data
Large Language Model
Instructions
Q: Is there a cycle
in this graph? 
 
Q: Is there a path
between
node 0 and node 4?
...
Response: Yes.
Task
 Info
...
Task-based
Questioner
Task
 Info
Task-specific Query
In a directed graph with 7 nodes numbered from 0 to 6:
node 0 should be visited before node 2,
...
This task is to find a valid topological sorting for this
directed graph.
Please provide a possible topological ordering path, for
example: 0 -> 1 -> 2 -> 3 -> 4. Q: The topological order
of the directed graph is:
VLM   Reasoner
              Task-agnostic descriptions
 
Response: 0 -> 2 -> 6 -> 3 -> 1 -> 4 -> 5.
Frozen Image Styles
Customizable Image Styles
             Task Responsibility
             Output Specification
              Refined descriptions 
(w/ the meanings of nodes/edges)
           
Figure 1: The architecture of the GITA framework with comparison to existing LLM solution.
basic image styles common to general images (i.e., size, resolution, backdrop) and only introducing
customizable variations in four graph-related image styles unique to visual graphs (i.e., layout, node
shapes, node outline styles, and edge thickness). Graph visualization in V can be formulated by the
following equation:
IG = V (G, Γ, ∆),
(1)
where IG denotes the visual graph derived from graph G, while Γ and ∆are the fixed basic image
styles and customizable graph-related image styles, respectively.
Visualizing the entire graph can be challenging when the number of nodes or edges is very large,
affecting the clarity of the images. To address this, our graph visualizer adopts the standard strategy of
k-hop subgraph sampling. Specifically, k-hop subgraph sampling for a node u in the set of vertices C
involves selecting a subgraph Gu = {Cu ⊆Nk(u), Eu ⊆E}, where Nk(u) includes nodes within
k steps from u and each edge (i, j) in Eu connects nodes within Cu. To generate the visual graph
of the k-hop subgraph Gu centered on u, the nodes within Gu are relabeled from 0 to |Cu| −1 to
facilitate the generalization of visual graphs. Subsequently, this relabeled subgraph Gu is fed to the
graph visualizer to generate its visual graph IGu by Eq. (1).
In practice, the graph visualizer can be implemented by a variety of graphic visualization tools, such as
Graphviz [17], Matplotlib [56], and NetworkX [22]. Among them, Graphviz can automatically design
the layouts of visual graphs, and is especially suitable for building large-scale datasets. Matplotlib is
excellent for customizable plots with fine-grained control, and NetworkX excels in complex network
analysis. We have implemented various graph visualizers using modular, plug-in architecture in
GITA. Specific examples of the visual graphs generated with these tools can be found in Appendix D.
Graph Describer. The graph describer D is tasked with generating task-agnostic textual descriptions
of a given graph G. To ensure clarity and fidelity of these descriptions, we meticulously craft a
curated set of graph-describing templates. The graph description templates outlined in Appendix E
are designed to cover a broad spectrum of scenarios, accommodating various graph configurations
including directed or undirected graphs and those with or without node or edge weights. To generate
the description for a given graph, the graph describer initially selects an appropriate template based
on the graph’s characteristics, such as its directionality and whether it includes node attributes or edge
weights. Subsequently, this template is used by replacing placeholders with actual data, such as the
number of nodes, the number of edges, and the endpoints of each edge, to craft detailed descriptions
tailored to the specific graph in question. The process for D to generate textual descriptions can be
formulated as follows:
DG = D(G, P),
(2)
where DG denotes the textual description generated by graph describer, and P is the graph-describing
template of the graph G.
By introducing these unified and structured graph-describing templates, the graph describer is
empowered to generate coherent and informative descriptions that focus on the inherent characteristics
of the graph itself, independent of specific task requirements.
Questioner. The questioner Q is tailored to capture the intricate requirements of specific tasks
and reflect them in its output task-specific query. In detail, Q receives the task-agnostic textual
4

descriptions from the graph describer and refines them to align with the task context by elucidating
the concrete meanings of nodes and edges. These refined descriptions are then enriched with task
responsibilities and input/output specifications to form task-specific queries. The formulation of the
questioner to generate the task-specific queries can be represented as follows:
QT
G = Q(T, DG),
(3)
where QT
G represents the task-specific query generated by the questioner with given the task require-
ment T and the textual description DG. The construction of task-specific queries can be approached
in two main ways: manual template-based construction and bootstrapping LLM agents. Manual
template-based construction enriches DG with task-specific manual templates, which is preferred for
tasks with precise requirements, such as the Traveling Salesman Problem (TSP) [12], where accuracy
is critical and the task definitions are well-understood. This is because it can ensure clarity and reduce
the risk of errors due to its meticulous attention to details. On the other hand, bootstrapping LLM
agents for automated synthesis is more economical and suitable for dynamic or bespoke tasks, such as
robotic planning or complex gaming scenarios, as it can take advantage of the speed and adaptability
of LLM agents to interpret context and generate appropriate queries, minimizing manual effort and
enhancing responsiveness to changing conditions. Both methods are illustrated with examples in
Appendix F, showcasing their applications and benefits in different scenarios.
VLM Reasoner. The VLM reasoner Rϕ performs final graph reasoning with visual inputs IG from
V and textual inputs QT
G from Q, and outputs responses in natural language. This reasoning process
can be represented as the following:
A = R(IG, QT
G),
(4)
where A is the answer generated by the vision-language model R. In this work, we adopt GPT-4V
and LLaVA-7B/13B as VLM reasoners. These models are regarded as representatives in the realm of
closed-source and open-source VLMs, respectively.
In summary, GITA systematically incorporates the vision modality into instruction-based graph
reasoning. In Appendix B, we discuss the characteristics of GITA, in aspects of generalizability,
flexibility and user-friendliness.
3.3
Visual Graph Augmentation
Visual graphs generated for the same graph G can be considered as an unique data augmentation
technique. Building on the four graph-related image styles introduced in the graph visualizer part
of Sec 3.2, we propose the following augmentation strategies: layout augmentation, node shape
augmentation, node outline style augmentation, and edge thickness augmentation. Specifically,
layout augmentation involves altering the layout styles while keeping all the other settings constant.
Similarly, by changing only the respective attributes, we can implement node shape augmentation,
node outline style augmentation, and edge thickness augmentation. These four proposed augmentation
strategies facilitate studies on the importance of each in enhancing the graph reasoning abilities of
VLM reasoners.
3.4
Training
Given a visual graph IG and a text-specific query QT
G, along with the target answer At, the VLM
reasoner of GITA is trained to generate answers A. Specifically, IG is input into the vision encoder of
the VLM reasoner, resulting in a set of visual features F. If there is a dimension difference between
Fv and the pretrained word embeddings, these F will be aligned with the pretrained word embedding
space of the text decoder by a vision-to-text projector. Finally, the aligned visual features Faligned
and QT
G are concatenated as input sequences of the text decoder.
Formally, given IG, QT
G, and At, the VLM reasoner is trained by minimizing the following negative
log-likelihood:
Lϕ = −
|A|
X
i=1
log pϕ(Ai | Faligned, QT
G, A<i),
(5)
where ϕ is the trainable parameter and Ai denotes the prediction token at the i-th position. Besides,
A<i represents the first i −1 predicted tokens. During the inference process, GITA is capable of
accepting structure graphs as inputs and performing graph reasoning in an end-to-end manner.
5

4
GVLQA Dataset
In this section, we introduce the GVLQA dataset to fill the absence of a vision-language-based
general graph reasoning dataset. It is designed to: 1) evaluate the graph reasoning capabilities of
VLMs or LLMs; 2) help models acquire fundamental graph comprehension and reasoning abilities as
a pretraining dataset.
4.1
Construction
The GVLQA dataset is created by utilizing the graph visualizerthe graph describer, and questioner in
GITA to generate vision-language-based question-answer pairs for graph reasoning on an open-source
graph dataset. Specifically, we first extract both the original graph structures and the ground-truth
outputs from the NLGraph-full dataset [59]. Then the graph visualizer (detailed in Sec 3.2) and
the graph describer (outlined in Sec 3.2) are used to generate visual graphs and textual descriptions
for these original graph structures, respectively. Afterwards, the questioner (described in Sec 3.2)
further improves and enriches the textual descriptions by converting them into textual queries. At the
same time, it transforms the ground-truth output into text-based answers, following specific output
requirements. By combining these visual graphs, textual queries, and text-based answers, we obtain
the Graph-based Vision-Language Question Answering (GVLQA) dataset.
In the process of establishing GVLQA, we employed graphviz [17] to instantiate the graph visualizer.
This choice is made due to its multitude of pre-defined layout algorithms, which enable convenient
adjustment of visual graph layouts. Additionally, manual template-based constructed queries are
utilized as the questioner because these tasks are famous with well-defined requirements.
4.2
Structure
The GVLQA dataset comprises 526K samples, each consisting of a visual graph, a textual query,
and its corresponding answer. It is divided into five subsets: GVLQA-BASE, and four augmentation
subsets GVLQA-AUGLY, GVLQA-AUGNS, GVLQA-AUGNO, and GVLQA-AUGET. In GVLQA-
BASE, the visual graphs are uniformly styled. The remaining four augmentation subsets are derived
from GVLQA-BASE through the four visual graph augmentations (Sec 3.3), varying in six different
layouts, three node shapes, four node outline styles, and four degrees of edge thickness, respectively.
Detailed statistics of the four subsets are shown in Table 6 of Appendix C.
Each GVLQA subset undergoes evaluation across seven graph reasoning tasks, outlined as follows.
• Connectivity [52] (denoted Connect): Determine whether two randomly selected nodes u
and v in an undirected graph are connected.
• Cycle [52]: Identify whether a cycle exists in an undirected graph.
• Topological Sort [35] (denoted TS): Find a valid topological sort for a directed acyclic
graph. Here, topological sort outputs a linear ordering of the nodes such that for every
directed edge u ←v, node u comes before v in the ordering.
• Shortest Path [13] (denoted SP): Find the shortest path between two nodes in a weighted
undirected graph. The shortest path between two nodes is the path connecting the two nodes
with the minimum sum of edge weights along the path.
• Maximum Flow [15] (denoted MaxFlow): Calculate the maximum flow from a source node
to a sink node in a network graph.
• Bipartite Graph Matching [36] (denoted BGM): Find a matching set in a bipartite graph
with the largest number of edges. A matching set is a collection of edges in which no two
edges share any common node.
• Hamilton Path [20] (denoted HP): Find a valid Hamilton path in an undirected graph. A
Hamiltonian path is a path that traverses each node in a graph exactly once.
Figure 6 offers illustrations for these tasks in the GVLQA-BASE dataset. Illustrations of all the
GVLQA subsets are provided in Appendix H.
6

5
Experiments
In this section, we extensively evaluate the performance of LLM baselines and the proposed GITA
on the GVLQA-BASE and five real-world datasets. To better clarify the reasoning capabilities of
solely visual graphs, we also test GITA without the textual descriptions of graphs, which can be
considered as a variant of GITA and denoted as vision-only (VO). In this case, the visual graph is the
only information source for graph reasoning. Additionally, we investigate the importance of visual
graph augmentation (Sec 3.3) strategies, by comparison GITA-7B trained on GVLQA-BASE and on
the other augmentation subsets of GVLQA (Sec 4.2). Lastly, we investigate the effectiveness of using
GVLQA as the pretrained dataset on real-world datasets. The evaluation metrics for all experiments
are accuracy by exact matching. For the fine-tuning setting, we fine-tune the LoRA adapters [28] for
all weight matrices in the text decoder of the VLM reasoner, while keeping the vision encoder in the
VLM reasoner frozen. More detailed experimental settings are in Appendix G.
5.1
Evaluation on the GVLQA-BASE Dataset
In this subsection, we perform experiments on the GVLQA-BASE dataset to compare GITA with
popular LLMs including GPT-4 Turbo [49], LLaMA2-7B/13B [57], and Vicuna-7B/13B [73], under
both zero-shot and fine-tuning settings. The experimental results are shown in Table 1. Based on
these results, we can obtain the following observations.
Observation 1: GITA Outperforms LLM Baselines. As can be seen in Table 1, GITA consistently
outperforms the LLM baselines under the same setting. This underscores its SOTA effectiveness in
instruction-based graph reasoning tasks, showing robust capabilities across different parameter scales
under both fine-tuning and zero-shot settings. Moreover, under the fine-tuning setting, incorporating
the vision modality consistently benefits 7B models. But for the 13B models, the performance of
some tasks may degrade. This could be attributed to the greater challenge of aligning representations
of the visual and textual modalities in the larger 13B models compared to the 7B models, in the
case of only fine-tuning LoRA adapters in the text decoder. We speculate that full training could
potentially address this issue. However, we leave this as future work due to resource constraints.
Observation 2: Mainstream Open-source VLM/LLMs Lack Fundamental Graph Reasoning
Abilities. The zero-shot results illustrate that prominent open-source LLMs or VLMs, including
LLaMA2, Vicuna, and LLaVA, exhibit minimal graph reasoning capabilities on the GVLQA-BASE
dataset. Specifically, these models produce random answers, i.e., randomly responding with either
"Yes." or "No." for tasks involving Connect and Cycle, resulting in a performance close to 50%. Cur-
Table 1: Accuracy (%) comparisons on GVLQA-BASE under zero-shot and fine-tuning settings,
where “VO” denotes a variant of GITA using only the vision modality.
Models
Connect
Cycle
TS
SP
MaxFlow
BGM
HP
Avg
Zero-shot
LLaMA2-7B
50.06
49.43
0.00
0.00
0.00
0.00
0.00
14.21
Vicuna-7B
50.06
49.43
0.00
0.00
0.00
0.00
0.00
14.21
GITA-7B (VO)
50.06
50.33
0.00
0.00
0.00
0.00
0.00
14.34
GITA-7B
50.06
49.43
0.00
0.00
0.00
0.00
0.00
14.21
GPT-4 Turbo
76.70
49.51
19.59
35.35
6.89
42.11
47.04
39.60
GITA-ZS (VO)
57.76
63.34
5.34
4.88
1.59
46.60
10.74
27.18
GITA-ZS
82.58
51.46
19.71
37.69
6.00
52.21
50.00
42.81
Fine-tuning
LLaMA2-7B
97.33
94.63
33.26
26.01
9.56
90.86
23.95
53.66
Vicuna-7B
97.58
95.04
34.46
25.98
9.33
91.04
25.55
54.15
GITA-7B (VO)
59.97
96.34
13.30
5.72
2.89
93.01
1.11
38.91
GITA-7B
98.95
96.67
41.12
32.15
20.00
93.19
29.26
58.76
LLaMA2-13B
98.79
93.36
33.83
27.93
12.22
91.34
33.46
55.85
Vicuna-13B
99.35
94.39
36.73
28.53
11.34
92.65
34.81
56.83
GITA-13B (VO)
58.00
96.91
14.45
5.72
4.89
93.19
1.85
39.29
GITA-13B
99.14
95.60
38.69
40.47
20.66
92.12
33.33
60.00
7

Table 2: Accuracy (%) comparisons across GVLQA subsets using GITA-7B (VO). ↑denotes dramatic
performance improvement.
Connect
Cycle
TS
SP
MaxFlow
BGM
HP
Avg
GVLQA-BASE
59.97
96.34
13.30
5.72
2.89
93.01
1.11
38.91
GVLQA-AUGNS
59.85
96.75
14.17
6.61
3.78
91.58
1.48
39.17
GVLQA-AUGNO
54.87
96.50
14.29
5.54
3.94
92.83
1.11
38.44
GVLQA-AUGET
57.98
96.91
13.37
5.97
3.11
91.76
0.74
38.55
GVLQA-AUGLY
87.18 ↑
97.07
14.86
76.55 ↑
3.94
93.19
70.74 ↑
63.36 ↑
Table 3: Accuracy (%) comparisons on real-world datasets under zero-shot and fine-tuning settings,
where ‡ indicates the usage of a checkpoint pretrained in the Cycle task of GVLQA-BASE.
Models
ca-GrQc
ca-HepTh
PolBlogs
Cora
CiteSeer
Avg
Zero-shot
LLaMA2-7B
40.59
48.89
10.74
24.35
30.33
30.98
Vicuna-7B
41.35
50.00
8.72
26.94
29.13
31.22
GITA-7B
71.95
86.06
46.98
31.37
30.63
53.40
GITA-7B‡
72.02
86.08
48.32
32.10
31.83
54.07
Fine-tuning
LLaMA2-7B
76.57
89.06
80.54
83.76
73.27
80.64
Vicuna-7B
78.95
89.85
80.54
84.87
74.17
81.68
GITA-7B
79.70
91.13
84.56
85.24
75.07
83.14
GITA-7B (w/ AUGLY)
79.77
91.21
85.23
85.24
75.68
83.43
GITA-7B‡
80.46
91.68
85.23
86.35
76.57
84.06
rent SOTA closed-source LLMs or VLMs, including GPT-4 Turbo and GPT-4V, demonstrate superior
zero-shot performance compared with the aforementioned open-source models. This observation
implies that current open-source LLMs and VLMs lack basic graph reasoning ability, which may be
attributed to the insufficient availability of relevant training data. Such observation also enhances our
motivation to propose the GVLQA dataset, with the aim of improving the graph reasoning capabilities
of VLMs/LLMs.
Observation 3: Increasing Model Size Leads to Better Graph Reasoning Capabilities. The
comparison of VLMs/LLMs with different parameter sizes, specifically 7B and 13B models, verify
the benefits of increasing the model size for graph reasoning capabilities. In this regard, GITA-13B
outperforms its counterpart with 7B parameters (GITA-7B) both on average and across four of the
seven tasks. However, it is worth noting that GITA-13B does not outperform GITA-7B on the other
three tasks. We hypothesize that this discrepancy may be attributed to insufficient modality alignment
due to LoRA fine-tuning.
Observation 4: Vision and Text Modalities Proficient in Different Types of Graph Reasoning
Tasks. We explore the individual capabilities of the visual and textual modalities within the GITA
framework. The results indicate that the text and vision modalities can complement each other
and contribute to better performance than individual ones, as removing either modality leads to
performance drops in most cases (Vicuna & GITA (VO) and GPT-4 Turbo & GITA (VO) in Table 1).
While the graph reasoning capability provided by the vision modality may not be as strong as that of
the text modality in most cases, relying solely on vision still enables the model to possess basic graph
reasoning abilities. Specifically, the model outperforms text-based LLMs in 2 of the 7 tasks (Cycle
and BGM) when relying solely on vision. This consistent improvement across all comparison groups
demonstrates the potential of the vision modality to excel in certain graph reasoning tasks, leveraging
its ability to capture visual patterns like cycles and graph properties such as bipartition. In contrast,
text exhibits a higher proficiency than vision modality in sequence-related graph reasoning problems,
particularly on tasks such as TS, SP, and HP, which require constructing ordered node sequences.
5.2
Evaluation for the Visual Graph Augmentations
To assess the impact of the proposed visual graph augmentation strategies (including layout, node
shape, node outline style, and edge thickness augmentations), we compare the performance of vision-
only GITA-7B models trained on the four augmented subsets of GVLQA and on GVLQA-BASE
8

(without augmentation). The results are presented in Table 2. To fully utilize the visual information
in visual graphs, we fine-tune the visual encoder of VLMs in addition to the vision-to-text projector
and the LoRA adapters within the text decoder in this experiment.
As can be seen from the results, a significant enhancement in overall performance is observed with
the introduction of layout augmentation (GVLQA-AUGLY). The average performance improves
remarkably from 38.91% to 63.36%. Notably, significant improvements are observed on SP (5.72%
to 76.55%), HP (1.11% to 70.74%), and Connect (59.97% to 87.18%). These findings highlight the
critical role of layout augmentation in generating visual graphs. In other words, this observation
suggests the potential for creating larger-scale datasets for vision-language-based graph reasoning,
which could significantly contribute to advancing this field. Conversely, the other three augmentations
do not yield such substantial performance improvements, further emphasizing the importance of
layout augmentation in vision-language-based reasoning.
5.3
Evaluation on Real-World Datasets
In this section, we study the effectiveness of GITA on the ca-GrQC [42] and ca-HepTh [42] datasets
for the link prediction task, and on the PolBlog [1], Cora [65] and CiteSeer [65] datasets for the node
classification task. Table 8 in the appendix C presents the statistics of these datasets. The graph
can have thousands of nodes/edges, making it infeasible to feed the entire graph into the model.
Consequently, we employ k-hop subgraph sampling (with k = 2) discussed in Sec 3.2 to satisfy the
token length restriction of LLMs and visual graph scope effectively.
The experimental results are presented in Table 3. It is evident that GITA consistently outperforms the
LLM baselines, and its performance progressively improves with the addition of layout augmentation
and the use of the GVLQA checkpoint. Notably, we emphasize the advantages of using GVLQA-
BASE as a pretrained dataset by comparing it with GITA-7b. Performance improvements of 0.67%
and 0.92% are observed in the zero-shot and fine-tuning settings, respectively. This highlights the
potential application value of the proposed GVLQA dataset.
5.4
Comparison of GITA with Dedicated Graph Baselines
Table 4: Accuracy (%) comparisons among dedicated GNNs and GITAs on GVLQA-Base.
Connect
Cycle
TS
SP
MaxFlow
BGM
HP
Avg
GCN
79.65
70.89
45.71
44.56
56.44
76.70
32.22
58.02
SAGE
82.72
73.58
44.51
49.25
50.67
81.00
36.67
59.78
GITA-7B
98.95
96.67
41.12
32.15
20.00
93.19
29.26
58.76
GITA-13B
99.14
95.60
38.69
40.47
20.66
92.12
33.33
60.00
Though GITA is designed for language-based general graph reasoning settings, which are much
more user-friendly (by user-readable natural language) and general (unique model architecture for
various scenarios) than the typical application of dedicated GNNs, it remains essential to conduct
a comprehensive comparison with specialized GNNs to elucidate the strengths and limitations of
GITA’s applicability and capabilities. To this end, we assess the graph reasoning abilities of GITA
against dedicated GNNs, including GCN [38] and SAGE [24], using the GVLQA-Base dataset, as
detailed in Table 4. In addition, we explore and compare the effects of k-hop subgraph sampling
on the proposed GITA and GNN baselines. Using the ca-Hepth dataset, we analyze the impact of
increasing the number of hops k on the reasoning time and performance of both GITA and GNNs.
The results are in Table 5.
Overall Graph Reasoning Ability Comparison. As shown in Table 4, compared to the dedicated
GNNs, the fine-tuned GITA-7B models have comparable average graph reasoning performance, with
the larger GATA-13B model performs slightly better. In particular, compared to GNNs, the GITA
model shows a stronger ability in recognizing local structures in the graphs (Connect and Cycle) and
to accomplish tasks with obvious layout heuristics (BGM). We believe that this advantage comes from
GITA’s visual perception. For SP and MaxFlow, GITA’s performance is inferior to GNNs. This may
be because GNNs process edge weight more effectively through the message-passing mechanism.
Scalability and Performance Variation with Different Numbers of Hops k. The inference time
results are shown in Table 5. As can be seen, GITA demonstrates inferior scalability compared
to the GNN baselines. Its scalability remains stable as the sampled graph size (i.e., k) increases.
9

Table 5: Accuracy (%) and Inference Time (in parenthe-
ses) for GNNs and GITA on ca-Hepth Dataset with dif-
ferent subgraph sampling hop number k ∈{1, 2, 3, 4}.
GCN
SAGE
GITA-7B
k=1
93.27 (0.02s)
94.40 (0.03s)
90.33 (17.23min)
k=2
94.49 (0.04s)
94.43 (0.04s)
91.13 (17.66min)
k=3
91.10 (0.04s)
90.95 (0.18s)
90.31 (17.22min)
k=4
81.92 (0.05s)
83.60 (0.22s)
86.10 (17.01min)
From the accuracy results in Table 5, GITA,
GCN, and SAGE achieve their peak perfor-
mance at k = 2, suggesting that a small
sampled graph size suffices for optimal per-
formance. Though the dedicated GNNs
attain higher peak performance than GITA,
they exhibit performance declines as k in-
creases (e.g., 3 or 4), while GITA’s perfor-
mance is more stable w.r.t. k.
5.5
Case Study
Textual Query
In an undirected graph,
(i,j) means that node i
and node j are
connected with an
undirected edge.
The nodes are
numbered from 0 to 9,
and the edges are: (9,4)
(1,3) (8,1) (4,0) (4,7)
(2,7) (5,7) (2,8) (6,9)
Is there a cycle in this
undirected graph?
Visual  Graph
Ground Truth: No.
Vicuna-7B: Yes.
GITA-7B (VO): No.
GITA-7B: No.
(a)
Textual Query
In an undirected graph,
the nodes are numbered
from 0 to 6, and the
edges are:
an edge between node 0
and node 5 with weight 1,
an edge between node 2
and node 0 with weight 1
...
Q: Give the shortest path
from node 4 to node 0:
Visual Graph
Ground Truth: 4->6->0.
Vicuna-7B: 4->6->0.
GITA-7B (VO): 4->5->0.
GITA-7B: 4->6->0.
(b)
Figure 2: A comparative case study of graph repre-
sentation in vision and text modalities. All meth-
ods are trained on the GVLQA-BASE dataset.
In this section, we present examples of graph
information provided in both visual and textual
formats, which offer some intuitive interpreta-
tions for our experimental results. Figure 2 (a)
shows an example where the GITA-7B (VO)
model outperforms its LLMs-based counterpart,
and Figure 2 (b) shows an opposite scenario.
The task depicted in Figure 2(a) is cycle detec-
tion and the correct answer is ‘No’. This is
predicted successfully by the vision-only GITA-
7B model, while the text-based Vicuna-7B fails.
In this example, recognizing cycle patterns is
much easier in visual graphs, whereas text-based
LLMs struggle with disordered textual descriptions of edges, which could inherently involve greater
complexity and more challenges.
On the other hand, the fixed layout of visual graphs presented in GVLQA-BASE may impede the
visual encoder in identifying the shortest path between two nodes, although we have verified layout
augmentation can greatly improve the graph reasoning abilities of models, as shown in Sec. 5.2. This
limitation might arise from the confusion caused by the visual distance within an image, without
considering the weights between the nodes. For instance, in Figure 2(b), the correct answer is
’4->6->0’, which visually appears as a more convoluted path but numerically has a shorter path
length of 3 = 1 + 2. In contrast, the incorrect answer given by GITA-7B (vision-only) is ’4->2->0’,
which has a higher path length cost of 4 = 1 + 3 but visually seems like a more direct shortcut.
This observation further validates the effectiveness of employing layout augmentation to enhance
performance in this task. Layout variations of visual graphs play a crucial role in mitigating the visual
confusion caused by the spatial arrangement within a visual graph. However, it seems more effective
for text-based LLMs to handle explicitly separated nodes and weights, as illustrated by the text (in
red) in Figure 2(b).
6
Conclusion
In this paper, we propose an end-to-end framework called GITA for vision-language-based graph
reasoning. Extensive experiments validate the superiority of incorporating visual information into
instruction-based graph reasoning. Furthermore, we conduct comparative analysis of the four
proposed visual graph augmentations and identify layout augmentation as the most effective approach
for enhancing visual graphs. This finding offers valuable insights for the development of larger-
scale datasets aimed at facilitating vision-language-based graph reasoning. Lastly, we highlight the
potential application value of the proposed GVLQA dataset as a pretrained dataset.
Acknowledgements
This work was supported by NSFC key grant 62136005 and NSFC general grant 62076118, and
in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grants
16200021 and 16202523).
10

References
[1] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided
they blog. In Proceedings of the 3rd international workshop on Link discovery, pages 36–43,
2005.
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in Neural Information Processing Systems,
35:23716–23736, 2022.
[3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
39–48, 2016.
[4] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. Advances in neural information
processing systems, 29, 2016.
[5] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018.
[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. Advances in neural information
processing systems, 26, 2013.
[7] Qi Cao, Huawei Shen, Jinhua Gao, Bingzheng Wei, and Xueqi Cheng. Popularity prediction on
social platforms with coupled graph neural networks. In Proceedings of the 13th International
Conference on Web Search and Data Mining, pages 70–78, 2020.
[8] Dexiong Chen, Leslie O’Bray, and Karsten Borgwardt. Structure-aware transformer for graph
representation learning. In International Conference on Machine Learning, pages 3469–3489.
PMLR, 2022.
[9] Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. Llaga: Large
language and graph assistant. arXiv preprint arXiv:2402.08170, 2024.
[10] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang,
Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (LLMs)
in learning on graphs. arXiv preprint arXiv:2307.03393, 2023.
[11] W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi. Instructblip:
Towards general-purpose vision-language models with instruction tuning. Advances in Neural
Information Processing Systems, 2023.
[12] George Dantzig, Ray Fulkerson, and Selmer Johnson. Solution of a large-scale traveling-
salesman problem. Journal of the operations research society of America, 2(4):393–410,
1954.
[13] Edsger W Dijkstra. A note on two problems in connexion with graphs. Numerische Mathematik,
1959.
[14] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for
large language models. In Proceedings of International Conference on Learning Representations,
2024.
[15] Lester Randolph Ford and Delbert R Fulkerson. Maximal flow through a network. Canadian
Journal of Mathematics, 1956.
[16] Shuai Fu, Xiequn Wang, Qiushi Huang, and Yu Zhang. Nemesis: Normalizing the soft-prompt
vectors of vision-language models. In Proceedings of International Conference on Learning
Representations, 2024.
11

[17] Emden R Gansner and Stephen C North. An open graph visualization system and its applications
to software engineering. Software: practice and experience, 30(11):1203–1233, 2000.
[18] Anne-Claude Gavin, Patrick Aloy, Paola Grandi, Roland Krause, Markus Boesche, Martina
Marzioch, Christina Rau, Lars Juhl Jensen, Sonja Bastuck, Birgit Dümpelfeld, et al. Proteome
survey reveals modularity of the yeast cell machinery. Nature, 440(7084):631–636, 2006.
[19] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning,
pages 1263–1272. PMLR, 2017.
[20] Ronald J Gould. Advances on the hamiltonian problem–a survey. Graphs and Combinatorics,
2003.
[21] Jiayan Guo, Lun Du, and Hengyu Liu.
GPT4Graph: Can large language models under-
stand graph structured data? an empirical evaluation and benchmarking.
arXiv preprint
arXiv:2305.15066, 2023.
[22] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and
function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos,
NM (United States), 2008.
[23] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems, 30, 2017.
[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems, 30, 2017.
[25] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn:
Simplifying and powering graph convolution network for recommendation. In Proceedings of
the 43rd International ACM SIGIR conference on research and development in Information
Retrieval, pages 639–648, 2020.
[26] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural
collaborative filtering. In Proceedings of the 26th international conference on world wide web,
pages 173–182, 2017.
[27] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi.
Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph repre-
sentation learning. In Proceedings of International Conference on Learning Representations,
2024.
[28] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu
Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference
on Learning Representations, 2021.
[29] Chao Huang, Huance Xu, Yong Xu, Peng Dai, Lianghao Xia, Mengyin Lu, Liefeng Bo, Hao
Xing, Xiaoping Lai, and Yanfang Ye. Knowledge-aware coupled graph neural network for social
recommendation. In Proceedings of the AAAI conference on artificial intelligence, volume 35,
pages 4115–4122, 2021.
[30] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 6700–6709, 2019.
[31] Hawoong Jeong, Sean P Mason, A-L Barabási, and Zoltan N Oltvai. Lethality and centrality in
protein networks. Nature, 411(6833):41–42, 2001.
[32] Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James
Kwok. Forward-backward reasoning in large language models for mathematical verification. In
Findings of the Association for Computational Linguistics, 2024.
[33] Weisen Jiang, Yu Zhang, and James Kwok. Effective structured-prompting by meta-learning
and representitive verbalizer. In International Conference on Machine Learning, 2023.
12

[34] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2901–2910, 2017.
[35] Arthur B Kahn. Topological sorting of large networks. Communications of ACM, 1962.
[36] Richard M Karp, Umesh V Vazirani, and Vijay V Vazirani. An optimal algorithm for on-line
bipartite matching. In Proceedings of the twenty-second annual ACM symposium on Theory of
computing, 1990.
[37] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016.
[38] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016.
[39] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, 42(8):30–37, 2009.
[40] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Létourneau, and Prudencio Tossou.
Rethinking graph transformers with spectral attention. Advances in Neural Information Pro-
cessing Systems, 34:21618–21629, 2021.
[41] Jure Leskovec, Lars Backstrom, Ravi Kumar, and Andrew Tomkins. Microscopic evolution
of social networks. In Proceedings of the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 462–470, 2008.
[42] Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and
shrinking diameters. ACM transactions on Knowledge Discovery from Data (TKDD), 1(1):2–es,
2007.
[43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image
pre-training with frozen image encoders and large language models. In International conference
on machine learning, 2023.
[44] Chang Liu and Bo Wu. Evaluating large language models on graphs: Performance insights and
comparative analysis. arXiv preprint arXiv:2308.11224, 2023.
[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances
in Neural Information Processing Systems, 2023.
[46] Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao
Dong, and Jie Tang. Mask and reason: Pre-training knowledge graph transformers for complex
logical queries. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 1120–1130, 2022.
[47] Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167–
256, 2003.
[48] OpenAI. GPT-3.5. Technical report, 2022.
[49] OpenAI. GPT-4 Turbo. Technical report, 2023.
[50] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou,
and Jonathan Halcrow. Let your graph do the talking: Encoding structured data for llms. arXiv
preprint arXiv:2402.05862, 2024.
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021.
[52] Robert Sedgewick. Algorithms in C, part 5: graph algorithms. Pearson Education, 2001.
13

[53] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural
tensor networks for knowledge base completion. Advances in neural information processing
systems, 26, 2013.
[54] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for
reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 6418–6428, 2019.
[55] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.
Graphgpt: Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023,
2023.
[56] Sandro Tosi. Matplotlib for Python developers. Packt Publishing Ltd, 2009.
[57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[58] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua
Bengio, et al. Graph attention networks. stat, 1050(20):10–48550, 2017.
[59] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
Can language models solve graph problems in natural language? In NeurIPS, 2023.
[60] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He,
and Tat-Seng Chua. Learning intents behind interactions with knowledge graph for recommen-
dation. In Proceedings of the web conference 2021, pages 878–887, 2021.
[61] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language
models. In Proceedings of Neural Information Processing Systems, 2022.
[62] Yanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok. Kicgpt: Large language model with
knowledge in context for knowledge graph completion. In Findings of the Association for
Computational Linguistics: EMNLP 2023, pages 8667–8683, 2023.
[63] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 2020.
[64] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
[65] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning
with graph embeddings. In International conference on machine learning, pages 40–48. PMLR,
2016.
[66] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language
is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.
[67] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,
Zhenguo Li, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap your own mathematical
questions for large language models. In Proceedings of International Conference on Learning
Representations, 2024.
[68] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:
Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 6720–6731, 2019.
[69] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for
relational and analogical visual reasoning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 5317–5327, 2019.
14

[70] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mm-
llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601,
2024.
[71] Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed
for learning graph representations. arXiv preprint arXiv:2001.05140, 2020.
[72] Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. Cone: Cone embeddings
for multi-hop reasoning over knowledge graphs. Advances in Neural Information Processing
Systems, 34:19172–19183, 2021.
[73] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Sto-
ica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In NeurIPS (Datasets and
Benchmarks Track), 2023.
[74] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompt-
ing enables complex reasoning in large language models. In Proceedings of International
Conference on Learning Representations, 2023.
[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592, 2023.
15

A
Visual Modality Enhances Effectiveness by Uncovering Critical
Substructures
In this section, we present a case study to highlight the complementary role of the visual modality in
graph reasoning tasks. The visual modality excels at recognizing beneficial substructures or local
patterns, which are crucial for effective graph reasoning. For instance, identifying the "hop number"
is essential for shortest path calculations, recognizing "leaf nodes" is vital for topological sorting,
and detecting "cycles" is necessary to avoid in Hamilton path construction. We extracted these
substructures from the GVLQA-Base dataset and manually labeled them. By employing a frozen
Vision Transformer (ViT) in the LLaVA framework with a trainable Multi-Layer Perceptron (MLP)
decoder, we achieved identification accuracies of 89.92%, 95.16%, and 92.39% for hop number
counting, leaf node identification, and cycle detection, respectively. In contrast, using a pre-trained
BERT model with the same trainable MLP decoder resulted in significantly lower accuracies of
55.47%, 26.33%, and 60.32% for the same tasks. Therefore, the enhanced effectiveness of integrating
visual and textual modalities can be attributed to the additional structural information provided by the
visual modality, which facilitates the identification of these critical substructures.
B
Advantages of GITA Over Traditional Graph Neural Networks
GITA offers several advantages over traditional Graph Neural Networks (GNNs) in terms of general-
izability, flexibility, and user-friendliness:
Unlike GNNs, which require task-specific feature engineering and architecture adjustments, GITA
employs a unified model architecture for all tasks, demonstrating its generalizability. By separating
task specifications from graph structures, GITA can handle various graph reasoning tasks seamlessly.
Additionally, it exhibits strong zero-shot capabilities, allowing it to perform well on tasks it has not
been explicitly trained on, which is a feature not commonly found in traditional GNNs.
Besides, traditional GNNs often demand specialized knowledge in model architectures and coding to
accommodate diverse tasks, posing a challenge for non-experts. In contrast, GITA overcomes this
barrier by employing language-based templates for task adaptation, enhancing its flexibility. This
flexibility enables GITA to effectively handle a broad spectrum of tasks, offering a framework that
can be customized to specific requirements using daily language, without the necessity of profound
expertise in graph neural networks.
Moreover, by leveraging existing VLMs, GITA can respond in natural language, allowing for intuitive
graph reasoning with simple queries like "Is there a cycle in this graph?" This stands in contrast
to the unreadable vector representations typically used in GNNs, significantly enhancing GITA’s
user-friendliness.
C
Datasets Statistics
Table 6: Statistics of the GVLQA dataset.
Subset
Connect
Cycle
TS
SP
MaxFlow
BMG
HP
Total
BASE
16,410
4,100
2,910
1,560
1,500
1,860
900
29,240
AUGLY
98,460
24,600
17,460
9,360
9,000
11,160
5,400
175,440
AUGNS
49,230
12,300
8,730
4,680
4,500
5,580
2,700
87,720
AUGNO
65,640
16,400
11,640
6,240
6,000
7,440
3,600
116,960
AUGET
65,640
16,400
11,640
6,240
6,000
7,440
3,600
116,960
Total
295,380
73,800
52,380
28,080
27,000
33,480
16,200
526,320
D
Illustrations for Visualization Tools in Graph Visualizer
The GITA graph visualizer incorporates a variety of implementations for existing visualization tools
such as Graphviz, Matplotlib with NetworkX, and igraph, each selected for their unique capabilities
16

Table 7: Average numbers of nodes and edges for each task in GVLQA.
Average / Task
Connect
Cycle
TS
SP
MaxFlow
BGM
HP
#nodes
25.01
23.42
21.86
13.65
13.90
21.13
13.24
#edges
95.46
23.66
114.10
23.99
49.16
51.03
45.05
Table 8: Statistics of real-world datasets
ca-GrQC
ca-HepTh
PolBlogs
Cora
CiteSeer
# Nodes
5,242
9,877
1,490
2,708
3,327
# Edges
14,496
25,998
19,025
5,278
4,676
domain
collaboration
collaboration
social
citation
citation
average degree
5.53
5.26
25.54
3.9
2.74
in graph rendering. These tools are implemented in our code as interchangeable modules, enhancing
flexibility based on the requirements of different projects.
Figure 3: Examples of the visual graph generated by various visualization tools.
Figure 3 showcases some visual graphs produced by these different graph visualizer implementations.
E
Graph-describing Templates
The graph describer relies on a set of unified structured templates designed to generate coherent and
informative descriptions that emphasize the inherent characteristics of the graph itself, regardless
of specific task requirements. These graph-describing templates cover various scenarios, includ-
ing directed graphs, undirected graphs, graphs with node identities or features, and graphs with
edge weights or capacities. Table 9 provides an illustration of these templates, where [P] denotes
placeholders required to be filled by corresponding graph information.
F
Examples of Manual-template-based and LLM-agent-bootstrapped Query
Generation
Manual-template-based Query Generation. The queries QT
G can be generated by task-specific man-
ual templates. These templates are manually crafted by human to supplement descriptions/instructions
about 1) concrete meanings of nodes and edges, 2) task responsibilities and 3) input/output specifica-
tions into the task-agnostic graph description DG. Therefore, the precision and faith of generated
task-specific queries QT
G are guaranteed by human calibrations. An example of manual-template-
based query generation for topological sorting is illustrated in Figure 4. In this example, placeholders
[P] are used to represent information that scripts will automatically fill in.
17

Graph categories
Undirected
Directed
Prototype
In an undirected graph, (i,j) means that node i and node j are
connected with an undirected edge. The nodes are numbered
from [P] to [P], and the edges are:
([P], [P]) , ([P], [P])...
In a directed graph, (i,j) means that node i and node j are
connected with a directed edge from node i to node j. The
nodes are numbered from [P] to [P], and the edges are:
([P], [P]) , ([P], [P])...
W/ Node Attributes In an undirected graph, the nodes are numbered from [P] to
[P], and every node has an attribute. (i,j) means that node i
and node j are connected with an undirected edge.
The attributes of nodes are:
node [P]: [P]
node [P]: [P]
...
The edges are: ([P],[P]) ([P],[P]) ...
In a directed graph, the nodes are numbered from [P] to [P],
and every node has an attribute. (i,j) means that node i and
node j are connected with a directed edge from node i to node
j.
The attributes of nodes are:
node [P]: [P]
node [P]: [P]
...
The edges are: ([P],[P]) ([P],[P]) ...
W/ Edge Weights
In an undirected graph, the nodes are numbered from [P] to
[P], and the edges are:
an edge between node [P] and node [P] with weight [P],
an edge between node [P] and node [P] with weight [P],
...
In a directed graph, the nodes are numbered from [P] to [P],
and the edges are:
an edge from node [P] to node [P] with weight [P],
an edge from node [P] to node [P] with weight [P],
...
W/ Both
In an undirected graph, the nodes are numbered from [P] to
[P], and every node has an attribute.
The attributes of nodes are:
node [P]: [P]
node [P]: [P]
...
And the edges are:
an edge between node [P] and node [P] with weight [P],
an edge between node [P] and node [P] with weight [P],
...
In a directed graph, the nodes are numbered from [P] to [P],
and every node has an attribute.
The attributes of nodes are:
node [P]: [P]
node [P]: [P]
...
And the edges are:
an edge from node [P] to node [P] with weight [P],
an edge from node [P] to node [P] with weight [P],
...
Table 9: Graph-describing Templates for various categories.
Textual Description
In a directed graph,
(i,j) means that node i and
node j are connected with
an undirected edge.
The nodes are  numbered
from 0 to 6, and the
edges are:
(0,2) 
(2,6) 
...
Task-specific Query
In a directed graph with 7
nodes numbered from 0 to 6:
node 0 should be visited before node
2,
...
This task is to find a valid topological
sorting for this directed graph.
Please provide a possible
topological ordering path, for
example: 0 -> 1 -> 2 -> 3 -> 4. Q:
The topological order of the directed
graph is:
             Task Responsibility
Manual-template-based Questioner
Manual Template:
In a directed graph with [P] nodes numbered from [P] to
[P]:
node [P] should be visited before node [P],
...
This task is to find a valid topological sorting for this
directed graph.
Please provide a possible topological ordering path, for
example: 0 -> 1 -> 2 -> 3 -> 4. Q: The topological order of
the directed graph is:
               meanings of nodes/edges
             Output Specification
              Task-agnostic textual description
Figure 4: Examples of the manual-template-based query generation, where [P] denotes the placehold-
ers.
18

LLM-agent-bootstrapped Query Generation. Figure 5 presents an example of employing a
bootstrapped LLM agent, such as ChatGPT[48], for monster-hunting gaming. By incorporating
task-specific information into the prompt, including node/edge meanings and task responsibilities,
the LLM agent automatically generates a response that serves as the desired task-specific query.
Compared to using manual templates, bootstrapping LLM agents for automated synthesis is more
flexible and economic as it can take advantage of LLM agents to automatically interpret context
and generate appropriate queries for various scenarios and minimize manual effort with changing
conditions. Such properties make it suitable for dynamic or bespoke tasks, such as robotic planning
or complex gaming scenarios.
Manual-template-based Questioner
Graph description: In a directed graph, (i,j) means that
node i and node j are connected with an undirected
edge. The nodes are numbered from 0 to 3, and the
edges are:
(0,1) (1,2) (2,3) ...
You are required to help refine the graph description with
1) concrete nodes/edges meaning: Node -> Monsters
Edge-> Two monsters coupled and hunting one of them
will irritate another. 2) task responsibility: hunting
monster each by each without irritating more than 1. 3) A
short sentence to rule input/output specification, for
example, a sentence like "The output should be only a
number" or "The output should be only Yes. or No.".
Directly give me the refined graph description:
Task-specific Query
In a directed graph, each node represents a monster, and an undirected
edge between two nodes signifies that the corresponding monsters are
coupled. If one monster is hunted, it will irritate the other monster it is
coupled with. The nodes are numbered from 0 to 3, and the edges are
defined as follows:
(0,1) - Monster 0 and Monster 1 are coupled.
(1,2) - Monster 1 and Monster 2 are coupled.
(2,3) - Monster 2 and Monster 3 are coupled.
...
The task responsibility is to hunt each monster one by one without
irritating more than one monster at a time.
Input/Output Specification: The output should be a sequence of numbers
representing the order in which monsters can be hunted without irritating
more than one monster at a time.
             Task Responsibility
               meanings of nodes/edges
             Output Specification
Prompt
               Task-agnostic textual description
Textual
Description
LLM Agent (ChatGPT)
Text-specific
information
Figure 5: Examples of the LLM-agent-bootstrapped query generation.
G
Experiment Settings
For all fine-tuning experiments, we use a batch size of 128 and adopt the AdamW optimizer (with a
learning rate of 0.0002 and 0.00002 for the LoRA adapters within the text decoder and vision-to-text
projector, respectively).
Detailed Settings for GVLQA Dataset During the evaluation, the temperature is set to 0 for all
baselines. All fine-tuning experiments are conducted on an NVIDIA DGX station with 8×A100 GPUs.
We split the GVLQA dataset in the ratio of 7:3 for training and testing, respectively. The accuracy
(%) metrics are computed by comparing the prediction and ground truths with exact matching. We
use the next-token-prediction loss to fine-tune the LoRA [28] adapters of LLMs and the vision-to-text
projector. Visual graphs are encoded as visual embeddings by a visual encoder. Visual embeddings
are concatenated with the embeddings of textual descriptions and instructions (i.e., questions), then
fed to the text decoder to generate the answer.
Real-world Datasets Here we provide more details about the five real-world datasets used in Sec
5.3. The datasets ca-GrQC and ca-HepTh represent collaboration networks from the arXiv sections
of General Relativity and Quantum Cosmology, and High Energy Physics - Theory, respectively,
featuring nodes as authors and edges as co-authorships. They can be downloaded from Stanford
Network Analysis Project (SNAP) website 6. PolBlogs is a network of U.S. political blogs from
February 2005, categorized by political alignment and linked by blog references. Cora and CiteSeer
are both citation networks, where nodes correspond to scientific papers and edges to citations, utilized
for tasks such as document classification and citation analysis, with papers categorized into various
research fields. Statistics of the datasets are shown in Table 8. For each dataset, 80%/10%/10% of
the edges are randomly used for training/validation/testing, respectively.
Detailed Settings for Real-world Benchmarks In the conventional semi-supervised node classi-
fication setting, class labels are available for some nodes, which is reflected in the visual graph by
coloring the nodes with a unique random color for each class. To focus on evaluating the model’s
ability to capture structural information, GITA filters out the influence of node features in Cora and
CiteSeer datasets. For link prediction tasks on ca-GrQC and ca-HepTh datasets, GITA treats the
graphs as undirected. In the test split, both the original links and their reverse links do not appear
in the train and valid splits. During training and evaluation, an equal number of negative sampled
links are used alongside the positive links. These negative links are sampled at each training epoch
6https://snap.stanford.edu/index.html
19

but remain fixed during evaluation. For the GVLQA pretrain checkpoint, GITA adopts the 7B cycle
checkpoint finetuned on GVLQA-BASE, where the performance is nearly mature. Hyperparameter
combinations for each model are determined through grid search, and the specific combinations can
be found in the provided code.
H
Illustrations of GVLQA subsets
In this section, we present the illustrations of the GVLQA subsets. Figure 6 provides an overview of
GVLQA-BASE. Subsequently, from Figure 7 to Figure 10, we showcase the augmented visual graphs
in GVLQA-AUGLY (augment layouts), GVLQA-AUGNS (augment node styles), GVLQA-AUGNO
(augment node outline styles), and GVLQA-AUGET (augment edge thickness), respectively.
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 5, and the
edges are: (0,5) (0,1)
(0,3) (1,5) (1,3) (2,4)
(3,5).
Q: Is there a path between node 1 and node 5
in this undirected graph?
A: Yes.
Visual graph
Textual description
1. Connectivity
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 4, and the
edges are: (1,0) (1,4)
(3,1) (2,0).
Q: Is there a cycle in this undirected graph?
A: No.
Visual graph
Textual description
2. Cycle
This diagram depicts
a directed graph, in
which each directed
edge from node A to
node B signifies that,
according to the
topological order,
node A must precede
node B. In this
directed graph with 5
nodes numbered from
0 to 4: node 1 should
be visited before
node 0 ...
Q: The topological order of the directed graph is:
A: 1,4,3,0,2.
Visual graph
Textual description
3. Topology Sort
This graphic illustrates
an undirected graph,
with each edge's
distance or length
indicated by a
numerical label in close
proximity. In a
undirected graph, the
nodes are numbered
from 0 to 4, and the
edges are: an edge
between node 1 and
node 0 with weight 1,
an edge ...
Q: Give the shortest path from node 3 to node 1:
A: 3->2->0->1.
Visual graph
Textual description
4. Shortest Path
This graphic illustrates a
directed graph, with each
edge's capacity indicated by a
numerical label in close
proximity. In this directed
graph, the nodes are
numbered from 0 to 5, and the
edges are: an edge from node
1 to node 3 with capacity 9, an
edge from node 2 to node 0
with capacity 9, an edge from
node 2 to node 3 with capacity
7, an edge from node 4 to node
3 with capacity 1, an edge from
node 4 to node 2 with ...
Q: What is the maximum flow from node 4 to node 3:
A: 14.
Visual graph
Textual description
5. Maximum Flow
There are 4 hosts
numbered from 0 to 3,
and 3 tasks numbered
from 0 to 2. Each host
has a set of tasks that
it is interested in: Host
2 is interested in task
2. Host 2 is interested
in task 1 ... However,
each host is capable of
solving only one
task, and similarly,
each task can be
resolved by just one
host.
Q: What is the maximum number of hosts that can be
assigned a task they are interested in?
A: 2.
Visual graph
Textual description
6. Bipartite Graph Matching
In this undirected graph,
(i,j) means that node i and
node j are connected with
an undirected edge. The
nodes are numbered from
0 to 10, and the edges are:
(7,1) (10,3) (0,6) (5,10)
(5,3) (9,8) (7,3) (1,10) (7,4)
(1,5) (4,10) (5,9) (7,5) (6,1)
(5,2) (2,8) (8,5) (4,2) (0,2)
(5,6) (1,4).
Q: Begin with node 0,  Is there a path in this graph
that visits every node exactly once? If yes, give the
path. Note that in a path, adjacent nodes must be
connected with edges.
A: 0->6->1->7->3->10->5->9->8->2->4.
Visual graph
Textual description
7. Hamilton Path
Figure 6: An overview of the GVLQA-BASE. Each figure depicts the tasks involving graph-based
reasoning, showcasing a visual graph, a textual question, and the corresponding answer.
I
Limitation
The GITA framework proposed in the paper, along with its experimental results, exhibits certain
limitations that should be acknowledged. Firstly, when dealing with large-scale graphs, the con-
ventional subgraph sampling strategy employed by GITA may result in imbalanced and insufficient
sampling, leading to the loss of critical graph structural information. This compromise is necessary
to accommodate the limited contextual length of the text-based LLM and the restricted scope of the
visual graph. Secondly, due to computational constraints, the fine-tuning procedures in the paper were
restricted to the LoRA framework. While this approach offers advantages, a more comprehensive
fine-tuning process that considers both visual and text modalities is expected to better align the two
and potentially enhance performance. Addressing these limitations should be considered as a future
research direction in this field.
20

In this undirected
graph, (i,j) means
that node i and
node j are
connected with an
undirected edge.
The nodes are
numbered from 0
to 5, and the edges
are: (0,5) (0,1)
(0,3) (1,5) (1,3)
(2,4) (3,5).
Q: Is there a path between node 1 and node 5
in this undirected graph?
A: Yes.
Visual graph
Textual description
1. Connectivity
Q: Is there a cycle in this undirected graph?
A: No.
Visual graph
Textual description
2. Cycle
This diagram depicts
a directed graph, in
which each directed
edge from node A to
node B signifies that,
according to the
topological order,
node A must precede
node B. In this
directed graph with 5
nodes numbered from
0 to 4: node 1 should
be visited before
node 0 ...
Q: The topological order of the directed graph is:
A: 1,4,3,0,2.
Visual graph
Textual description
3. Topology Sort
This graphic illustrates
an undirected graph,
with each edge's
distance or length
indicated by a
numerical label in close
proximity. In a
undirected graph, the
nodes are numbered
from 0 to 4, and the
edges are: an edge
between node 1 and
node 0 with weight 1,
an edge ...
Q: Give the shortest path from node 3 to node 1:
A: 3->2->0->1.
Visual graph
Textual description
4. Shortest Path
This graphic illustrates a
directed graph, with each
edge's capacity indicated by a
numerical label in close
proximity. In this directed
graph, the nodes are
numbered from 0 to 5, and the
edges are: an edge from node
1 to node 3 with capacity 9, an
edge from node 2 to node 0
with capacity 9, an edge from
node 2 to node 3 with capacity
7, an edge from node 4 to node
3 with capacity 1, an edge from
node 4 to node 2 with ...
Q: What is the maximum flow from node 4 to node 3:
A: 14.
Visual graph
Textual description
5. Maximum Flow
There are 4 hosts numbered
from 0 to 3, and 3 tasks
numbered from 0 to 2. Each
host has a set of tasks that it
is interested in: Host 2 is
interested in task 2. Host 2
is interested in task 1 ...
However, each host is
capable of solving only one
task, and similarly, each task
can be resolved by just one
host.
Q: What is the maximum number of hosts that can be
assigned a task they are interested in?
A: 2.
Visual graph
Textual description
6. Bipartite Graph Matching
In this undirected graph,
(i,j) means that node i and
node j are connected with
an undirected edge. The
nodes are numbered from
0 to 10, and the edges are:
(7,1) (10,3) (0,6) (5,10)
(5,3) (9,8) (7,3) (1,10) (7,4)
(1,5) (4,10) (5,9) (7,5) (6,1)
(5,2) (2,8) (8,5) (4,2) (0,2)
(5,6) (1,4).
Q: Begin with node 0,  Is there a path in this graph
that visits every node exactly once? If yes, give the
path. Note that in a path, adjacent nodes must be
connected with edges.
A: 0->6->1->7->3->10->5->9->8->2->4.
Visual graph
Textual description
7. Hamilton Path
W
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 4, and the
edges are: (1,0) (1,4)
(3,1) (2,0).
Figure 7: An overview of the GVLQA-AUGLY. Figures are akin to GVLQA-BASE but vary only in
layouts.
In this undirected
graph, (i,j) means
that node i and
node j are
connected with an
undirected edge.
The nodes are
numbered from 0
to 5, and the edges
are: (0,5) (0,1)
(0,3) (1,5) (1,3)
(2,4) (3,5).
Q: Is there a path between node 1 and node 5
in this undirected graph?
A: Yes.
Visual graph
Textual description
1. Connectivity
Q: Is there a cycle in this undirected graph?
A: No.
Visual graph
Textual description
2. Cycle
This diagram depicts
a directed graph, in
which each directed
edge from node A to
node B signifies that,
according to the
topological order,
node A must precede
node B. In this
directed graph with 5
nodes numbered from
0 to 4: node 1 should
be visited before
node 0 ...
Q: The topological order of the directed graph is:
A: 1,4,3,0,2.
Visual graph
Textual description
3. Topology Sort
This graphic illustrates
an undirected graph,
with each edge's
distance or length
indicated by a
numerical label in close
proximity. In a
undirected graph, the
nodes are numbered
from 0 to 4, and the
edges are: an edge
between node 1 and
node 0 with weight 1,
an edge ...
Q: Give the shortest path from node 3 to node 1:
A: 3->2->0->1.
Visual graph
Textual description
4. Shortest Path
Q: What is the maximum flow from node 4 to node 3:
A: 14.
Visual graph
Textual description
5. Maximum Flow
There are 4 hosts
numbered from 0 to 3,
and 3 tasks numbered
from 0 to 2. Each host
has a set of tasks that
it is interested in: Host
2 is interested in task
2. Host 2 is interested
in task 1 ... However,
each host is capable of
solving only one
task, and similarly,
each task can be
resolved by just one
host.
Q: What is the maximum number of hosts that can be
assigned a task they are interested in?
A: 2.
Visual graph
Textual description
6. Bipartite Graph Matching
In this undirected graph,
(i,j) means that node i and
node j are connected with
an undirected edge. The
nodes are numbered from
0 to 10, and the edges are:
(7,1) (10,3) (0,6) (5,10)
(5,3) (9,8) (7,3) (1,10) (7,4)
(1,5) (4,10) (5,9) (7,5) (6,1)
(5,2) (2,8) (8,5) (4,2) (0,2)
(5,6) (1,4).
Q: Begin with node 0,  Is there a path in this graph
that visits every node exactly once? If yes, give the
path. Note that in a path, adjacent nodes must be
connected with edges.
A: 0->6->1->7->3->10->5->9->8->2->4.
Visual graph
Textual description
7. Hamilton Path
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 4, and the
edges are: (1,0) (1,4)
(3,1) (2,0).
This graphic illustrates a
directed graph, with each
edge's capacity indicated by a
numerical label in close
proximity. In this directed
graph, the nodes are
numbered from 0 to 5, and the
edges are: an edge from node
1 to node 3 with capacity 9, an
edge from node 2 to node 0
with capacity 9, an edge from
node 2 to node 3 with capacity
7, an edge from node 4 to
node 3 with capacity 1, an
edge from node 4 to node 2
with ...
Figure 8: An overview of the GVLQA-AUGNO. Figures are akin to GVLQA-BASE but vary only in
node outline styles.
21

In this undirected
graph, (i,j) means
that node i and
node j are
connected with an
undirected edge.
The nodes are
numbered from 0
to 5, and the edges
are: (0,5) (0,1)
(0,3) (1,5) (1,3)
(2,4) (3,5).
Q: Is there a path between node 1 and node 5
in this undirected graph?
A: Yes.
Visual graph
Textual description
1. Connectivity
Q: Is there a cycle in this undirected graph?
A: No.
Visual graph
Textual description
2. Cycle
This diagram depicts
a directed graph, in
which each directed
edge from node A to
node B signifies that,
according to the
topological order,
node A must precede
node B. In this
directed graph with 5
nodes numbered from
0 to 4: node 1 should
be visited before
node 0 ...
Q: The topological order of the directed graph is:
A: 1,4,3,0,2.
Visual graph
Textual description
3. Topology Sort
This graphic illustrates
an undirected graph,
with each edge's
distance or length
indicated by a
numerical label in close
proximity. In a
undirected graph, the
nodes are numbered
from 0 to 4, and the
edges are: an edge
between node 1 and
node 0 with weight 1,
an edge ...
Q: Give the shortest path from node 3 to node 1:
A: 3->2->0->1.
Visual graph
Textual description
4. Shortest Path
Q: What is the maximum flow from node 4 to node 3:
A: 14.
Visual graph
Textual description
5. Maximum Flow
There are 4 hosts
numbered from 0 to 3,
and 3 tasks numbered
from 0 to 2. Each host
has a set of tasks that
it is interested in: Host
2 is interested in task
2. Host 2 is interested
in task 1 ... However,
each host is capable of
solving only one
task, and similarly,
each task can be
resolved by just one
host.
Q: What is the maximum number of hosts that can be
assigned a task they are interested in?
A: 2.
Visual graph
Textual description
6. Bipartite Graph Matching
In this undirected graph,
(i,j) means that node i and
node j are connected with
an undirected edge. The
nodes are numbered from
0 to 10, and the edges are:
(7,1) (10,3) (0,6) (5,10)
(5,3) (9,8) (7,3) (1,10) (7,4)
(1,5) (4,10) (5,9) (7,5) (6,1)
(5,2) (2,8) (8,5) (4,2) (0,2)
(5,6) (1,4).
Q: Begin with node 0,  Is there a path in this graph
that visits every node exactly once? If yes, give the
path. Note that in a path, adjacent nodes must be
connected with edges.
A: 0->6->1->7->3->10->5->9->8->2->4.
Visual graph
Textual description
7. Hamilton Path
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 4, and the
edges are: (1,0) (1,4)
(3,1) (2,0).
This graphic illustrates a
directed graph, with each
edge's capacity indicated by a
numerical label in close
proximity. In this directed
graph, the nodes are
numbered from 0 to 5, and the
edges are: an edge from node
1 to node 3 with capacity 9, an
edge from node 2 to node 0
with capacity 9, an edge from
node 2 to node 3 with capacity
7, an edge from node 4 to
node 3 with capacity 1, an
edge from node 4 to node 2
with ...
Figure 9: An overview of the GVLQA-AUGNS. Figures are akin to GVLQA-BASE but vary only in
node shapes.
In this undirected
graph, (i,j) means
that node i and
node j are
connected with an
undirected edge.
The nodes are
numbered from 0
to 5, and the edges
are: (0,5) (0,1)
(0,3) (1,5) (1,3)
(2,4) (3,5).
Q: Is there a path between node 1 and node 5
in this undirected graph?
A: Yes.
Visual graph
Textual description
1. Connectivity
Q: Is there a cycle in this undirected graph?
A: No.
Visual graph
Textual description
2. Cycle
This diagram depicts
a directed graph, in
which each directed
edge from node A to
node B signifies that,
according to the
topological order,
node A must precede
node B. In this
directed graph with 5
nodes numbered from
0 to 4: node 1 should
be visited before
node 0 ...
Q: The topological order of the directed graph is:
A: 1,4,3,0,2.
Visual graph
Textual description
3. Topology Sort
This graphic illustrates
an undirected graph,
with each edge's
distance or length
indicated by a
numerical label in close
proximity. In a
undirected graph, the
nodes are numbered
from 0 to 4, and the
edges are: an edge
between node 1 and
node 0 with weight 1,
an edge ...
Q: Give the shortest path from node 3 to node 1:
A: 3->2->0->1.
Visual graph
Textual description
4. Shortest Path
Q: What is the maximum flow from node 4 to node 3:
A: 14.
Visual graph
Textual description
5. Maximum Flow
There are 4 hosts
numbered from 0 to 3,
and 3 tasks numbered
from 0 to 2. Each host
has a set of tasks that
it is interested in: Host
2 is interested in task
2. Host 2 is interested
in task 1 ... However,
each host is capable of
solving only one
task, and similarly,
each task can be
resolved by just one
host.
Q: What is the maximum number of hosts that can be
assigned a task they are interested in?
A: 2.
Visual graph
Textual description
6. Bipartite Graph Matching
In this undirected graph,
(i,j) means that node i and
node j are connected with
an undirected edge. The
nodes are numbered from
0 to 10, and the edges are:
(7,1) (10,3) (0,6) (5,10)
(5,3) (9,8) (7,3) (1,10) (7,4)
(1,5) (4,10) (5,9) (7,5) (6,1)
(5,2) (2,8) (8,5) (4,2) (0,2)
(5,6) (1,4).
Q: Begin with node 0,  Is there a path in this graph
that visits every node exactly once? If yes, give the
path. Note that in a path, adjacent nodes must be
connected with edges.
A: 0->6->1->7->3->10->5->9->8->2->4.
Visual graph
Textual description
7. Hamilton Path
In this undirected graph,
(i,j) means that node i
and node j are connected
with an undirected edge.
The nodes are numbered
from 0 to 4, and the
edges are: (1,0) (1,4)
(3,1) (2,0).
This graphic illustrates a
directed graph, with each
edge's capacity indicated by a
numerical label in close
proximity. In this directed
graph, the nodes are
numbered from 0 to 5, and the
edges are: an edge from node
1 to node 3 with capacity 9, an
edge from node 2 to node 0
with capacity 9, an edge from
node 2 to node 3 with capacity
7, an edge from node 4 to
node 3 with capacity 1, an
edge from node 4 to node 2
with ...
Figure 10: An overview of the GVLQA-AUGET. Figures are akin to GVLQA-BASE but vary only in
edge thicknesses.
22

NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
• Delete this instruction block, but keep the section heading “NeurIPS paper checklist",
• Keep the checklist subsection headings, questions/answers and guidelines below.
• Do not modify the questions and only use the provided macros for your answers.
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Please refer to the abstract part and the contribution enumeration at the tail of
the introduction part.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
23

Justification: Please refer to Appendix I
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justification: The paper does not involve any theoretical result.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We show fundamental experiment settings in Section 5, and more details for ex-
periments settings in Appendix G. Besides, we provide the complete codes as supplementary
materials.
Guidelines:
24

• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The complete codes are included, and the proposed GVLQA dataset is released
with common access.
Guidelines:
• The answer NA means that the paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
25

• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Experiment details are given in both Section 5 and Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: The paper does not include error bars.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We report the machine (type and storage) requirements in Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
26

• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We make sure the research conducted in the paper conform, in every respect,
with the NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The research does not have concerns about societal impacts because it is
designed for general-purpose graph reasoning.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
27

Answer: [Yes]
Justification: The paper includes using an graph visualizer to generate abstract graph images,
however, these images are focus on graph structure, without any sensitive information.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have cited necessary assets and conduct CC-BY 4.0 for our codes and
datasets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The code and other supplementary materials are followed with readme and
instructions.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
28

14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
29
MICROADAM: Accurate Adaptive Optimization with
Low Space Overhead and Provable Convergence
Ionut-Vlad Modoranu1∗
Mher Safaryan1
Grigory Malinovsky2
Eldar Kurtic1
Thomas Robert1
Peter Richtárik2
Dan Alistarh1
1Institute of Science and Technology Austria (ISTA)
2King Abdullah University of Science and Technology (KAUST)
Abstract
We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called
MICROADAM that specifically minimizes memory overheads, while maintaining
theoretical convergence guarantees. We achieve this by compressing the gradient
information before it is fed into the optimizer state, thereby reducing its memory
footprint significantly. We control the resulting compression error via a novel
instance of the classical error feedback mechanism from distributed optimiza-
tion [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019] in which
the error correction information is itself compressed to allow for practical memory
gains. We prove that the resulting approach maintains theoretical convergence
guarantees competitive to those of AMSGrad, while providing good practical per-
formance. Specifically, we show that MICROADAM can be implemented efficiently
on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MI-
CROADAM provides practical convergence competitive to that of the uncompressed
Adam baseline, with lower memory usage and similar running time. Our code is
available at https://github.com/IST-DASLab/MicroAdam.
1
Introduction
The Adam [Kingma and Ba, 2014] adaptive optimizer and its variants [Reddi et al., 2019, Loshchilov
and Hutter, 2019] has emerged as a dominant choice for training deep neural networks (DNNs),
especially in the case of large language models (LLMs) with billions of parameters. Yet, its versatility
comes with the drawback of substantial memory overheads: relative to naive SGD-based optimization,
the Adam optimizer states doubles the memory overhead, as it requires storing two additional parame-
ters for each variable. For large-scale models, these memory demands pose a significant challenge. In
turn, this has spurred research into memory-efficient adaptive optimizers, such as AdaFactor [Shazeer
and Stern, 2018], 8-bit Adam [Dettmers et al., 2021], or the very recent GaLore [Zhao et al., 2024]
low-rank projection approach. Despite their popularity and practical utility, the above methods lack
rigorous convergence guarantees, and often trade off memory reductions with decreased convergence
in practice. This raises the question of whether it is possible to design adaptive optimizers that are
not only memory-efficient, but also maintain strong theoretical and practical performance metrics.
Contributions. In this paper, we address this gap by introducing MICROADAM, an adaptive optimizer
which guarantees low memory usage but also ensures provable convergence. We develop our approach
to improve the performance of finetuning LLMs and mainly focus on the research question “are all
gradient entries important for optimization?” To answer this question, we start from the idea that
we can allow the (lossy) sparse projection of gradient information before it enters the optimizer
states; crucially, different from prior work, we ensure convergence by correcting for the inherent error
∗Correspondence to ionut-vlad.modoranu@ista.ac.at
38th Conference on Neural Information Processing Systems (NeurIPS 2024).

due to compression by employing a novel variant of error correction, a mechanism introduced for
distributed optimization [Seide et al., 2014]. However, simply using error feedback would not lead to
memory savings, since the size of the error correction buffer is comparable to the that of the original
optimizer state. Instead, our main algorithmic innovation is in showing that the error feedback can
itself be compressed in the context of adaptive optimization. This renders the memory overhead of
error feedback negligible, while preserving convergence guarantees.
Specifically, on the theoretical side, we provide a new analysis showing that, under reasonable
assumptions on the loss function being optimized and on the degree of compression, MICROADAM
provably guarantees convergence, at asymptotically the same rate as AMSGrad [Zhou et al., 2024a],
i.e. a version of Adam with general convergence guarantees, that fixes a fundamental technical issue
in the Adam optimizer’s proof [Reddi et al., 2019]. The key finding is that our approach allows for the
overhead of compression to be shifted to the higher-order terms, where it should not impact practical
convergence in common cases. This claim holds both for general smooth non-convex functions, and
for non-convex functions under the Polyak-Lojasiewicz (PL) assumption, highlighting a trade-off
between the degree of compression of the gradients, and that of the error feedback.
We complement our algorithmic and analytic results with an efficient GPU implementation of
MICROADAM, which we validate for fine-tuning language models from the BERT [Devlin et al.,
2018], OPT [Zhang et al., 2022] and LLaMA [Touvron et al., 2023] families, with hundreds of
millions to billions of parameters. We show that, in practice, gradients can be projected to very high
sparsity (99%), while the error correction can be stored at 4 bits per component, without loss of
convergence. Concretely, our method can significantly improve upon the memory footprint of the
extremely popular 8bit Adam [Dettmers et al., 2021] when fine-tuning models such as LLaMA2-
7B/13B [Touvron et al., 2023], at similar or better accuracy. At the same time, MICROADAM provides
better accuracy relative to high-compression heuristics such as GaLore [Zhao et al., 2024].
In summary, we provide a new theoretically-grounded approach to memory-efficient adaptive op-
timization, which has the advantage of providing both theoretical guarantees and good practical
convergence, while being scalable to billion-parameter models. MICROADAM could therefore serve
as a useful new tool for accurate and memory-efficient optimization of large models.
2
Related Work
We mainly focus on related work reducing the cost of optimizer states. Dettmers et al. [2021]
considers this problem, specifically by performing fine-grained quantization of the optimizer states.
Their work does not alter the Adam algorithm; instead, it deals with the challenge of accurately
compressing the dynamically-changing meta-data sequence. As the name suggests, the space savings
correspond to roughly halving the memory required by the optimizer states, relative to FP16. In
the same vein, AdaFactor [Shazeer and Stern, 2018] and CAME [Luo et al., 2023] reduce memory
cost by factorizing the second-order statistics, while the recent GaLore [Zhao et al., 2024] factorizes
the gradients themselves before they enter the optimizer state (but does not use error correction).
Importantly, these methods are heuristics: they do not provide theoretical guarantees under standard
assumptions,2 and in practice require careful tuning to preserve convergence [Luo et al., 2023]. By
contrast, our method is theoretically justified, and provides good practical convergence. Earlier work
by Anil et al. [2019] provides convergence guarantees for a compressed variant of Adagrad [Duchi
et al., 2010] called SM3, improving upon earlier work by Spring et al. [2019]. However, it is not
clear how to extend their approach to the popular Adam optimizer, and heuristic methods appear to
provide superior performance [Luo et al., 2023].
Conceptually, our work is related to error feedback mechanisms studied in distributed optimization,
e.g. [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019, Richtárik et al., 2021].
Specifically, Li et al. [2022] proved convergence of AdaGrad-like algorithms in conjunction with
error feedback, in a distributed environment. Our focus is different: minimizing memory costs in the
single-node setting: for this, we show that the error correction buffer can itself be compressed. We
provide an analysis for the resulting new algorithm, and efficient CUDA implementations.
More broadly, scaling adaptive or second-order optimizers to large models is a very active area.
Works such as GGT [Agarwal et al., 2019], Shampoo [Gupta et al., 2018] and M-FAC [Frantar et al.,
2GaLore [Zhao et al., 2024] does state convergence guarantees for a variant of the algorithm with fixed
projections, but this is under a strong “stable rank” assumption, which may not hold in practice.
2

2021] provided quadratic-space algorithms that are still feasible to execute for moderate-sized DNNs,
but will not scale for billion-parameter models. Follow-up work such as AdaHessian [Yao et al.,
2020], Sophia [Liu et al., 2023], Sketchy [Feinberg et al., 2023] and EFCP [Modoranu et al., 2023],
scaled these approaches via additional approximations. Of these, the closest work to ours is EFCP,
which uses sparsification plus standard error feedback to compress the gradient window employed in
the Fisher approximation of the Hessian. However, EFCP does not compress the error accumulator,
assumes a different optimization algorithm (Natural Gradient [Amari, 2016]), lacks convergence
guarantees, and does not scale to billion-parameter models.
3
The MICROADAM Algorithm
Notation. We consider a standard Adam-type algorithm, which we will augment for memory savings.
We will use f for the loss function, d for the model size, k for the gradient density (sparsity d −k),
θt and gt for the model parameters and gradient at step t respectively, ηt for the learning rate, λ for
the weight decay parameter, mt and vt for the first and second moment of the gradient, ϵ for the
numerical stability constant, β1 and β2 for the momentum coefficients for mt and vt respectively.
Furthermore, we use et for the error feedback (EF) vector, b the number of bits for EF quantization,
m for the sliding window size, G = (I, V) for the sliding window of size m × k that stores indices I
and values V selected by the Top-K operator.
Algorithm Description. We provide pseudocode in Algorithm 1 and highlight the parts related to
error feedback quantization in blue. The main idea is to compress the gradients via TopK sparsification
before they enter the optimizer state, and to correct for the inherent error by applying error feedback
et ∈Rd. Instead of storing the optimizer state directly, we maintain a “sliding window” of highly-
sparse past gradients and dynamically re-compute the Adam statistics at each step based on this
window. Yet, this alone does not improve space, as the error buffer partially negates the benefits of
gradient compression. Instead, we prove that the error feedback accumulator can itself be compressed
via quantization.
In detail, at step t = 1, the error feedback e1 is completely zero, as initialized in line 2, and thus,
at line 5 the accumulator a1 will only contain the stochastic gradient g1. At line 6, we perform the
Top-K compression and only keep the top-1% of values V1 and their corresponding indices I1. The
compression is equivalent to choosing the top-1% values in the left and right tails (outliers) due
to the absolute value we apply on top of accumulator a. At line 7, we remove the outliers from
the accumulator because they will be transferred to the buffer matrix G. This step is equivalent to
e ←a −Tk(a) found in theoretical results. After line 7, what is left in a is called the error feedback
(e.g. the weights which were not chosen by Top-k). At line 8, we compute the statistics δ and ∆
needed for quantization, and at line 9, we effectively quantize the accumulator (e.g. error feedback
after line 7). At line 10 we update the buffer G, in lines 11, 12 and 13 we compute the statistics ˆm, ˆv
(computed by squaring the entries of G element-wise) and update the model parameters.
For steps t ≥2, the only change compared to t = 1 is that error feedback e is not zero anymore.
Since the error is compressed, we need to decompress it and add it to the gradient. This process
happens at line 5 and it is the point where we feed back the error: the accumulator will store the
gradient whose direction is corrected by the error (e.g. the cumulative history of weights not chosen
by Top-k at the previous steps).
Properties and Limitations. We would like to point out that the resulting update ut = mt/(ϵ+√vt)
will always be highly sparse when the window size m is small. For illustration, if we use density
k = d/100 (e.g. 1% density equivalent to 99% sparsity) with m = 10 and suppose that all rows in
the indices matrix I are disjoint, then the overall density in the update ut will be 90%. The sparsity of
ut increases if rows in I have common values. MICROADAM yields good results for LLM finetuning
and pre-training computer vision models, as the experimental section shows. However, we noticed
the update ut of MICROADAM is too sparse to be able to provide good enough updates for LLM
pre-training. We believe this happens because the attention layers must receive dense updates to be
able to learn the correlations between words.
Dynamic Statistics. In ADAMSTATS procedure in Algorithm 2 we implement the unrolled recursion
of momentum zt ←βzt−1+(1−β)gt for the last m sparse gradients as zt ←(1−β) Pt
i=t−m βt−igi
and we also perform the bias correction in the end. Because we compute ˆmt and ˆvt using the last
m sparse gradients in the window, in line 4 we dynamically determine the exponent r for the decay
factor βr based on the current optimization step t, ith row of the circular buffer G and the window size
3

m. The last gradient added to G will have r = 0, while the oldest gradient in G will have r = m −1.
In the end, we will add the values βrVi to the buffer z at the corresponding indices Ii, which is a
fast operation because we only manipulate 1% of values at a time. We discuss the efficient CUDA
implementation in the Appendix.
Algorithm Intuition. To gain intuition, we illustrate the impact of compressing gradients via TopK
before they are incorporated into the optimizer state for Adam, both with and without error feedback
(EF). Figure 1 shows how EF fixes AdamW with Top-K compression. The plot on the left shows the
optimization trajectory of the original Adam optimizer. The center plot illustrates the convergence of
Top-K Adam when we only choose the largest coordinate from the accumulator (equivalent to 50%
sparsity since the problem is 2D). In the end, on the right side we show that adding EF to Top-K
Adam recovers the same optimization trajectory as the original Adam optimizer. Extrapolating to
higher dimensional problems, our MICROADAM approach helps recover the trajectory of the original
Adam optimizer, while using less memory. The results clearly show that EF is essential for fast
convergence. Besides, TopK with EF, which is a surrogate of MICROADAM, allows for competitive
convergence relative to the uncompressed baseline. In Appendix F, we discuss the implications of
Error Feedback applied to GaLore.
Algorithm 1 Pseudocode for MICROADAM
with quantized EF
1: Input: β1, β2, ϵ, G, T, d, k
2: m0, v0 ←0d, 0d
δ1, ∆1 ←0, 0
e1 ←04b
d
3: for t = {1, 2, ..., T} do
4:
gt ←e∇θf(θt)
5:
at ←gt + Q−1(et, δt, ∆t)
6:
It, Vt ←Tk(|at|)
7:
at[It] ←0
8:
δt+1, ∆t+1 ←min(at), max(at)
9:
et+1 ←Q(at, δt+1, ∆t+1)
10:
Gi,: ←(It, Vt)
11:
ˆmt ←ADAMSTATS(β1, G)
12:
ˆvt ←ADAMSTATS(β2, G2)
13:
θt+1 ←θt −ηt
ˆmt
ϵ+√ˆvt
14:
i ←(i + 1)%m
15: end for
Algorithm 2 Adam Statistics, Quantization and
Inverse Quantization
1: procedure ADAMSTATS(β, G, t, m, d)
2:
z ←0d
3:
for i ∈{1, 2, ..., min(t, m)} do
4:
r ←(t −i −1)%m
5:
z[Ii] ←z[Ii] + βrVi
6:
end for
7:
return
(1−β)z
1−βt
8: end procedure
1: procedure Q(x, δ, ∆, b = 4)
2:
u ←∆−δ
2b−1
3:
xQ ←⌊x−δ
u
+ 1
2⌋
4:
return xQ
5: end procedure
1: procedure Q−1(xQ, δ, ∆, b)
2:
u ←∆−δ
2b−1
3:
x ←xQ · u + δ
4:
return x
5: end procedure
Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the
Rosenbrock function f(x, y) = (1 −x)2 + 100(y −x2)2 starting from (x0, y0) = (−1
2, 1). Notice
the extremely “jagged” profile of TopK-Adam without EF, and the recovered convergence when EF
is added.
3.1
Efficient Implementation
A direct implementation (e.g., in Pytorch) of the previous algorithm would not bring significant
benefits, and would in fact might slow down optimization in terms of wall-clock time. To realize the
theoretical gains, we detail a GPU-aware implementation below.
4

Accumulator at. First, we do not use an additional accumulator tensor at; instead, we use a CUDA
kernel to dequantize the error buffer, and store the result in the grad attribute of the model parameters.
This allows us to accumulate the error feedback into the gradients, without allocating a full or half
precision d-dimensional array. Each component of the EF has 4 bits and the entire EF is stored in an
array of size d/2 of uint8 values.
Top-K. Since we run on LLMs with billions of parameters, naive storage of the sparse indices would
require using an int64 type for the indices matrix I, assuming that the Top-K operator is applied
globally to all the parameters in at. To avoid this cost, we apply Top-K in blocks of fixed size
Bd < 215 −1 = 32767 and store block-relative indices in int16 format (during the development of
MICROADAM, PyTorch did not have support for uint16). Applying Top-K per row to at reshaped to
2D is not only faster, but provides the block-relative indices directly.
Computing Top-K in blocks also allows us to allocate and efficiently use CUDA shared memory
blocks to dynamically compute the statistics ˆmt and ˆvt for Adam, as described in the ADAMSTATS
procedure in Algorithm 2. We allocate the maximum possible shared memory for each thread block
and store ˆmt (first half) and ˆvt (second half) at consecutive locations in the shared memory. Once
these statistics are computed, it is easy to update the model parameters. Note that the block-relative
indices returned by Top-K will be directly used as indices in the shared memory array of CUDA
thread blocks to retrieve values from I and V.
Quantization metadata. Our approach also stores two additional vectors δ and ∆used for quantiza-
tion. Since the quantization block size Bq is set to a very large value (e.g. 100K), the space required
for these two arrays becomes negligible in comparison to the buffer G and error feedback e.
Practical memory usage. We note that we apply MICROADAM per layer, and that the size of
quantization statistics δ and ∆are allocated based on the layer size. Having many such small tensors
may result in slightly sub-optimal memory allocation from Pytorch. This is why our reported memory
usage can be higher than the theoretical usage for small models, in the 100M parameter range; these
effects disappear for billion-parameter models, where the savings are significant.
3.2
Memory footprint analysis for the optimizer states and comparison with other methods
We now compare the theoretical memory footprint of MICROADAM with AdamW [Loshchilov and
Hutter, 2019], AdamW-8 bits [Dettmers et al., 2021], and GaLore [Zhao et al., 2024], focusing
on memory usage of the optimizer states mt and vt, each of size d, expressed in bytes (B). For
concreteness, we report the practical memory usage for the optimizer state for a Llama-2 7B model
for each optimizer.
• AdamW stores states in float32 format (4 B per component), resulting in a total memory footprint
of 4d + 4d = 8d (B), while using bfloat16 would result in 4d (B) memory. We will refer to these
memory footprints as MAW 32 = 8d (B) = 50.21 (GB) and MAW 16 = 4d (B) = 25.10 (GB).
• AdamW-8 bit stores states in 8-bit format (1 B per component), both with d components, with
memory footprint of MAW 8 = d + d = 2d (B) = 12.55 (GB).
• MICROADAM stores the error feedback e in 4-bit format (0.5 B per component) and the sliding
window G that stores the indices I in int16 and V in bfloat16 format. Both have m×k components,
each requiring 2 B per component. In the end, for m = 10 and k = d/100, the memory footprint
is MµA(m) = 0.5d + 4mk (B) = 0.9d (B) = 5.65 (GB), that is, half of AdamW-8bit.
• GaLore. Given a neural network with L layers, where each layer has weight matrix Wi ∈RAi×Bi
(shaped as a 2D matrix), the model size d = PL
i=1 AiBi. GaLore uses a rank-r compression via
the SVD composition as Wi = USV T , where U ∈RAi×Ai and we choose the first r columns
of U as Ri ∈RAi×r to project the gradient of Wi to the r-dimensional space. As a consequence,
the dimension d shrinks to dr = PL
i=1 Air = r PL
i=1 Ai, which represents the total number of
components to be stored in the GPU memory only for the projection matrices Ri. If we suppose
they are stored in bfloat16 (2 B per component), then the entire memory usage for low-rank
projection would be 2dr. Note that some layers in the model are rank-1 and they do not need
compression, but will still have associated states in Adam, which means they must be tan into
consideration when computing the theoretical memory (we will use ϵ1 for memory of rank-1 layers).
In addition, we have to store the states mt and vt for AdamW in bfloat16 format, which adds
another 4dr bytes. In sum, the total memory footprint of GaLore is MGLAW 16(r) = 6dr +2ϵ1 (B),
while for the 8-bit version we get MGLAW 8(r) = 4dr + 2ϵ1 (B). In the end, the practical
5

memory usage for Llama-2 7B is MGLAW 8(256) = 1.36 (GB), MGLAW 8(1024) = 5.43 (GB),
MGLAW 16(256) = 2.04 (GB) and MGLAW 8(1024) = 8.15 (GB).
Discussion. Assume our goal is to obtain a lower memory footprint compared to AdamW-8 bit. We
fix the gradient density to k = d/100 and we have to determine the number of gradients (window
size) m for MICROADAM in order to be competitive with AdamW-8 bit.
For this, we have to solve the equation MµA(m) = MAW 8 for m, resulting in mmax = 37.5.
Specifically, if we use a gradient history of m < mmax gradients in G, MICROADAM will have
theoretical memory savings. We will see that, in practice, this history size m = 10 is more than
enough for good practical results. As entries in the window past this range are dampened extremely
significantly, their influence is negligible. In Appendix D, we provide Python code to compute the
theoretical memory usage for the three optimizers for Llama-2 7B.
4
Convergence Guarantees for MICROADAM
In this section, we present our theoretical framework. We begin by introducing and discussing the
analytical assumptions used in our theory, providing an analytical view of the proposed MICROADAM
algorithm, along with two theoretical convergence results.
4.1
Gradient and Error Compression
We now define two classes of compression operators widely used in the literature.
Assumption 1. The gradient compressor C : Rd →Rd is q-contractive with 0 ≤q < 1, i.e.,
∥C(x) −x∥≤q ∥x∥,
for any x ∈Rd.
The compression we use in Algorithm 1 is the TopK compressor Tk selecting top k coordinates in
absolute value. This is known to be contractive with q =
p
1 −k/d. Another popular contractive
compressor is the optimal low-rank projection of gradient shaped as a d × d matrix, in which case
q =
p
1 −R/d where R is the projection rank.
The second class of compressors, which we use for the error feedback, requires unbiasedness and
relaxes the constant in the uniform bound.
Assumption 2. The error compressor Q : Rd →Rd is unbiased and ω-bounded with ω ≥0, namely,
E[Q(x)] = x,
∥Q(x) −x∥≤ω ∥x∥,
for any x ∈Rd.
One example of ω-bounded compressor, a version of which is used in Algorithm 2, is the randomized
rounding quantizer we employ, whose properties we provide below.
Lemma 1. Consider Algorithm 2 with randomized rounding, i.e., for a vector x ∈Rd with δ =
mini xi and ∆= maxi xi, let ˆxi := ⌊xi−δ
u
+ ξ⌋u + δ be the i-th coordinate of the quantized vector
ˆx, where ξ ∼U[0, 1] is the uniform random variable and u = ∆−δ
2b−1 is the quantization level. Then
E[ˆx] = x,
∥ˆx −x∥≤
√d−2
2b−1
∆−δ
√
∆2+δ2 ∥x∥,
for all x ∈Rd.
Next, we provide an “analytical” view of our method in Algorithm 3. Essentially, we use the
contractive compressor C for compressing the error corrected gradient information gt + et, and the
unbiased compressor Q to compress the remaining compression error gt + et −C(gt + et).
Algorithm 3 MICROADAM: Analytical View
1: Input: parameters β1, β2 ∈(0, 1), ϵ > 0, step-size η > 0, θ1 ∈Rd, e1 = m0 = v0 = ˆv0 = 0d
2: for t = {1, 2, ..., T} do
3:
gt = e∇θf(θt)
⋄Compute unbiased stochastic gradient
4:
˜gt = C(gt + et)
⋄Add accumulated error et and compress
5:
et+1 = Q(et + gt −˜gt)
⋄Update and compress the error
6:
mt = β1mt−1 + (1 −β1)˜gt
⋄Update first-order gradient moment
7:
vt = β2vt−1 + (1 −β2)˜g2
t
⋄Update second-order gradient moment
8:
ˆvt = max(vt, ˆvt−1)
⋄Apply AMSGrad normalization
9:
θt+1 = θt −η
mt
√ˆvt+ϵ
⋄Update the model parameters
10: end for
6

It is clear from this description that our objective with these two compressors, C and Q, is to
approximate the dense gradient information gt + et using two compressed vectors: ˜gt = C(gt + et)
and Q(gt + et −˜gt). However, in doing so, we inevitably lose some information about gt + et
depending on the degree of compression applied to each term. Thus, the condition (1 + ω)q < 1
required by our analysis can be seen as preventing excessive loss of information due to compression.
4.2
Convergence Guarantees for General Smooth Non-convex Functions
Next, we state our algorithm’s convergence guarantees under standard assumptions, stated below:
Assumption 3 (Lower bound and smoothness). The loss function f : Rd →R is lower bounded by
some f ∗∈R and L-smooth, i.e., ∥∇f(θ) −∇f(θ′)∥≤L ∥θ −θ′∥, for any θ, θ′ ∈Rd.
Assumption 4 (Unbiased and bounded stochastic gradient). For all iterates t ≥1, the stochastic
gradient gt is unbiased and uniformly bounded by a constant G ≥0, i.e., E[gt] = ∇f(θt), ∥gt∥≤G.
Assumption 5 (Bounded variance). For all iterates t ≥1, the variance of the stochastic gradient gt
is uniformly bounded by some constant σ2 ≥0, i.e., E[∥gt −∇f(θt)∥2] ≤σ2.
Main Result. The above assumptions are standard in the literature, e.g. [Défossez et al., 2022, Li
et al., 2022, Xie et al., 2023, Zhou et al., 2024a]. Under these conditions, if the two compressors
satisfy the basic condition (1 + ω)q < 1, we show:
Theorem 1. (Non-convex convergence rate) Let Assumptions 1, 2, 3, 4, 5 hold and qω := (1+ω)q <
1. Then, choosing η = min{
ϵ
4LC0 ,
1
√
T }, MICROADAM (Algorithm 3) satisfies
1
T
PT
t=1 E[∥∇f(θt)∥2] ≤2C0

f(θ1)−f ∗
√
T
+ L(σ2+C2
2G2)
ϵ
√
T

+ O

G3(G+d)
T

with constants C0 :=
q
4(1+q2ω)3
(1−q2ω)2 G2 + ϵ and C2 := ωq(1 +
2qω
1−q2ω ).
Discussion. First, notice that the leading term
1
√
T of the rate is the optimal convergence speed
for non-convex stochastic gradient methods [Ghadimi and Lan, 2016]. Furthermore, the obtained
convergence rate O( 1
√
T + d
T ) asymptotically matches the rate of uncompressed AMSGrad in the
stochastic non-convex setup [Zhou et al., 2024a]. Hence, the added compression framework of
the MICROADAM together with error feedback mechanism can slow down the convergence speed
only up to some constants including the dimension. Evidently, the additional constants C0 and
C2 affected by compression and appearing in the leading terms can be easily estimated once the
compressors are fixed. Besides, if we store the full error information without applying Q compressor
(i.e., ω = 0, qω = q), then MICROADAM reduces to the single-node Comp-AMS method by Li et al.
[2022] recovering the same convergence rate. The full proof is provided in the Appendix.
4.3
Convergence Rate for Non-Convex Functions under the PL Condition
Next, we show that we can obtain even stronger bounds when the objective satisfies the PL condition:
Assumption 6 (PL-condition). For some µ > 0 the loss f satisfies Polyak-Lojasiewicz (PL) inequality
∥∇f(θ)∥2 ≥2µ(f(θ) −f ∗),
for any θ ∈Rd.
In this case, we can show:
Theorem 2. (PL convergence rate) Let Assumptions 1, 2, 3, 4, 5 and 6 hold, and qω < 1. Then,
choosing η = min{
ϵ
4LC0 , 2C0 log T
µT
}, MICROADAM (Algorithm 3) satisfies
E[f(θT +1)] −f ∗≤2 log T
T

LC2
0
µ
σ2+(C1+C2
2)G2
µϵ
+ C0(1+C1)(1+d)G2
µ√ϵ

+ e
O

G4(G+d)
T 2

with constant C1 :=
β1
1−β1 (1 + C2) +
2qω
1−q2
ω .
Discussion. In contrast to the general non-convex setup, the study of non-convex analysis under the
PL condition for AMSGrad or Adam-type methods is much less extensive. The only work we found
analyzing the PL condition, which claims to be the first in this direction, focuses on Adam when
β2 →1, achieving a convergence rate of O( 1
T ) [He et al., 2023]. However, our MICROADAM is based
on AMSGrad normalization, and no constraint on β2 is imposed in the analysis. Therefore, similar to
7

the general non-convex case, we are able to achieve the best-known convergence rate in the leading
term, up to a logarithmic factor. The third, higher-order term has higher constant dependencies, but
they should be negligible as the term is dampened by T 2. Hence, in this case as well, the theory
predicts that the convergence rate of the algorithm should be similar to the uncompressed version,
modulo a constant that can be controlled using the compression parameters.
5
Experiments
We now validate our optimizer experimentally. We focus on comparing MICROADAM with Adam,
Adam-8bit, GaLore and CAME in the context of LLM finetuning on different tasks and with SGD,
Adam and AdamW-8bit in the context of ResNets on ImageNet. Concretely, we test our optimizer in
full finetuning (FFT) scenario on BERT-Base/Large [Devlin et al., 2018] and OPT-1.3B [Zhang et al.,
2022] on GLUE/MNLI and Llama2-7B/13B [Touvron et al., 2023] on the GSM8k math reasoning
dataset and on the Open-Platypus instruction tuning dataset, as well as pre-training ResNet models
on ImageNet. We provide full details regarding training settings hyper-parameters in Appendix B.
Finetuning results on GLUE/MNLI. We first test our integration of MICROADAM in HuggingFace
Transformers [Wolf et al., 2020] on moderate-sized language models such as BERT-Base/Large
(110M and 335M parameters) and OPT-1.3B, comparing with Adam, Adam-8bit, CAME and GaLore.
The results are shown in Table 1. Certain optimizers, notably CAME and GaLore, had numerical
stability issues across runs; for a fair comparison, we report the numbers for the run with maximum
accuracy. We emphasize that all methods were tuned using the same protocol.
The results show that MICROADAM achieves comparable memory usage to the state-of-the-art
heuristics Adam-8bit and GaLore, while being surprisingly lower than CAME on all tasks. The
memory savings for GaLore are more visible when the model size increases, which follows our
analysis of theoretical memory usage. However, we see that these gains come at a significant accuracy
cost for GaLore: across all tasks, it drops at least 1% accuracy relative to MICROADAM. For BERT-
Base we ran GaLore with a higher SVD re-computation frequency T = 20 (10× lower) and the
results did not improve, but its running time was much higher. Relative to 8bit Adam, MICROADAM
uses essentially the same memory, but achieves slightly better accuracy.
From these results, we conclude that MICROADAM can provide better accuracy relative to other
memory-efficient methods on moderate-sized models, at similar space costs. We show training loss
curves in Appendix C.
Table 1: Finetuning results on GLUE/MNLI. We report the entire memory usage read from the GPU
during training, that includes the optimizer state, activations and gradients. The asterisk flags the runs
for which one or two seeds did not converge (we report the run with maximum performance).
Model
Metric
MICROADAM
(m = 10)
Adam
Adam-8b
CAME
GaLore
r = 256
train loss
0.2651
0.4228
0.3402
0.6431*
0.3908*
BASE
accuracy
85.10%
83.53%
84.61%
76.13%*
83.82%*
(110M)
memory
2.55 GB
2.70 GB
2.53 GB
2.69 GB
2.53 GB
train loss
0.2509
0.3857
0.2876
0.6658*
0.3768*
LARGE
accuracy
86.17%
84.79%
86.18%
75.23%*
84.90%*
(335M)
memory
5.98 GB
6.64 GB
6.04 GB
6.59 GB
5.85 GB
train loss
0.2122
0.2066
0.2611
0.4959
0.2831
OPT-1.3B
accuracy
88.18%
87.90%
87.81%
83.15%
87.70
(1.3B)
memory
15.28 GB
17.66 GB
15.00 GB
17.13 GB
13.66 GB
Finetuning results for LLaMA2 on GSM-8k. Next, we perform finetuning on Llama-2 7B/13B
on GSM-8k, a challenging grade-school-level mathematical reasoning dataset. The baseline model
obtains extremely low zero-shot accuracy on this task and therefore fine-tuning is necessary. In this
setup, we compare MICROADAM with Adam and Adam-8bit in terms of evaluation accuracy and
memory usage. In Table 2 we show our results for 3 training epochs, global batch size 32 with
micro-batch (per-device) size 1, max sequence length 512 on a single GPU, which are the standard
8

parameters for this task. We integrated our optimizer with the llm-foundry repository of MosaicML
and tested via lm-evaluation-harness.
For the 7B model, out results show that MICROADAM can allow accurate full fine-tuning of a 7B
model on this task using a single 40GB GPU. Moreover, MICROADAM preserves accuracy relative
to Adam, with lower memory usage than the well-optimized implementation of 8bit AdamW, and
marginally lower running time for the shorter gradient window m = 10. Increasing the window size
m to 20 gradients leads to slightly better accuracy, at the cost of higher runtime and space, but still in
the 40GB limit. Running GaLore in this setup was infeasible since using SVD decomposition for
all layers in the model was too slow. Preliminary experiments (with high runtimes) did not yield
competitive accuracy. We show training loss curves in Appendix C.
The results show that MICROADAM allows for full accuracy recovery on this task as well relative to
Adam, despite using 50% less memory. (The memory usage and runtime are very similar to those in
Table 2 and are therefore omitted from Table 3.) Moreover, MICROADAM obtains consistently better
accuracy relative to Adam-8b, especially on the more challenging ARC-c task.
Table 2: FFT results for Llama-2 7B/13B on GSM-8k.
LLaMA-2 size
Optimizer
Accuracy
State
Total
Runtime
Adam
34.50%
25.1 GB
55.2 GB
1h 17m
7B
Adam-8b
34.34%
12.55 GB
42.5 GB
1h 18m
MICROADAM (m = 10)
34.72%
5.65 GB
37.1 GB
1h 8m
MICROADAM (m = 20)
35.10%
8.25 GB
39.7 GB
1h 37m
Adam
47.08%
48.42 GB
>80 GB
1h 20m
13B
Adam-8b
45.19%
24.21 GB
>80 GB
1h 17m
MICROADAM (m = 10)
44.88 %
10.9 GB
70 GB
1h 38m
Finetuning results for LLaMA2-7B on Open-Platypus. Finally, in Table 3 we present FFT results
with various optimizers on the popular instruction-tuning Open-Platypus dataset [Lee et al., 2023].
To ensure fair comparisons, we perform the same grid search for each optimizer to find the best
performing learning-rate, while keeping all other hyperparameters at their default values. We use
m = 10 gradients for the sliding window and gradient density k = 1%. Evaluations are conducted
following the standard few-shot setup of the Open LLM Leaderboard [Beeching et al., 2023] on the
following datasets: ARC-c [Clark et al., 2018], HellaSwag [Zellers et al., 2019], MMLU [Hendrycks
et al., 2021], and Winogrande [Sakaguchi et al., 2019].
Table 3: FFT results on instruction-following Open-Platypus [Lee et al., 2023] dataset. The results
show that MICROADAM fully recovers accuracy relative to baseline Adam, and outperforms the 8bit
variant, despite using less memory.
Optimizer
Memory
Average
Accuracy
ARC-c
25-shot
HellaSwag
10-shot
MMLU
5-shot
Winogrande
5-shot
AdamW
67.17 GB
62.10
52.56
77.38
45.53
72.93
Adam-8b
53.93 GB
61.84
51.96
77.51
44.11
73.79
MICROADAM
46.63 GB
62.36
53.07
77.46
45.04
73.87
Pre-training results for ResNets on ImageNet. In Table 4 we present our results for pre-training
(from scratch, randomly initialized weights) for ResNet-18/50 on ImageNet (see Figure 6 and Figure 7
in Appendix C). We compare our MICROADAM with SGD, Adam and AdamW and report the training
loss, validation accuracy and only the optimizer state memory because the total memory usage is
not an issue for ResNets on ImageNet (here we focus on the results to emphasize the pre-training
performance). For ResNet-18, AdamW reaches the lowest training loss, followed by AdamW-8bit,
MICROADAM and in the end MICROADAM. However, despite slightly larger loss, MICROADAM
yields the best result among all optimizers, with 2% more than the highly tuned SGD, while having
the lowest memory footprint for the optimizer states. For ResNet-50, AdamW-8bit reaches the lowest
training loss, followed by AdamW, MICROADAM and SGD. The validation accuracy for AdamW
and AdamW-8bit is surprisingly small compared to SGD and MICROADAM. As it is widely known
9

in the community, Adam variants have lower performance than SGD for Computer Vision tasks
and MICROADAM fixes this issue (see the Discussion section for an intuitive explanation for this
phenomenon).
Table 4: Pre-training results for ResNet-18 and ResNet-50 on ImageNet.
Model
Metric
SGD
AdamW
AdamW-8bit
MICROADAM
Train Loss
1.416
1.087
1.104
1.218
ResNet-18
Accuracy
69.96%
69.83%
70.13%
71.86%
State Size
44.59 MB
89.18 MB
22.30 MB
10.03 MB
Train Loss
0.9770
0.5344
0.5158
0.7732
ResNet-50
Accuracy
76.24%
72.05%
72.48%
77.37%
State Size
97.49 MB
194.98 MB
48.75 MB
21.94 MB
Pre-training LLMs. In this section we explain why we do not include LLM pre-training results. Our
motivation is twofold. First, MICROADAM is mainly designed for low-memory finetuning, and the
experimental section shows that MICROADAM achieved this goal. Surprisingly, updating only 10%
of the weights at each step yields to significantly better performance compared to SGD for ResNets
on ImageNet. Secondly, our experiments on LLM pre-training showed difficulties in achieving the
same performance compared to AdamW-8bit. Our explanation is that projection matrices from the
attention layers must receive dense updates to learn the correlations between words. In contrast to the
convolutional filters for CV models, the weights in attention are much larger and try to capture global
correlations (features) between words, while the convolutional filters are smaller and capture local
features.
Discussion. In Appendix A we provide information about the optimization set, intuitive explanations
for the implicit regularization effect of MICROADAM, as well as an overview of our results.
6
Limitations and Broader Impact
The MICROADAM algorithm we propose is designed and tested with fine-tuning workloads in mind,
where the user aims to minimize the memory cost of optimizing over a powerful pre-trained model.
Additional work is needed to adapt our approach to the case of LLM pre-training, which presents a
different set of challenges, both in terms of implementation and optimization trajectory. We plan to
undertake this study in future work as the current implementation works for ResNets.
Another limitation we aim to address in future work is that we have only focused on sparsity as a
form of gradient projection. However, our theoretical analysis also applies to low-rank projection of
gradients. We believe that our practical implementation can be extended to this case as well, although
providing a general, accurate, and efficient implementation will require non-trivial efforts.
Our work introduces a new, accurate, and memory-efficient optimizer for fine-tuning LLMs. The
major positive impact of our approach is its ability to maintain performance while reducing memory
requirements, thereby lowering the cost of running experiments due to the reduced hardware expenses.
It is important to note that while our optimizer can enhance performance and reduce costs, we do not
have control over the neural network applications trained with it.
Acknowledgements
The authors thank Razvan Pascanu, Mahdi Nikdan and Soroush Tabesh for their valuable feedback,
the IT department from Institute of Science and Technology Austria for the hardware support and
Weights and Biases for the infrastructure to track all our experiments. Mher Safaryan has received
funding from the European Union’s Horizon 2020 research and innovation program under the Marie
Sklodowska-Curie grant agreement No 101034413.
References
N. Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang. Efficient full-matrix
adaptive regularization. In International Conference on Machine Learning, pages 102–110. PMLR,
2019.
10

D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat, and C. Renggli. The convergence
of sparsified gradient methods. In Advances in Neural Information Processing Systems, pages
5973–5983, 2018.
S.-i. Amari. Information geometry and its applications, volume 194. Springer, 2016.
R. Anil, V. Gupta, T. Koren, and Y. Singer. Memory efficient adaptive optimization. Advances in
Neural Information Processing Systems, 32, 2019.
E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and
T. Wolf. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_
llm_leaderboard, 2023.
P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you
have solved question answering? try arc, the ai2 reasoning challenge, 2018.
A. Défossez, L. Bottou, F. Bach, and N. Usunier.
A simple convergence proof of adam and
adagrad.
Transactions on Machine Learning Research, 2022.
ISSN 2835-8856.
URL
https://openreview.net/forum?id=ZPQhzTSWA7.
T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization.
arXiv preprint arXiv:2110.02861, 2021.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
Bert: Pre-training of deep bidirectional
transformers for language understanding, 2018.
J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Res., 12:2121–2159, 2010.
V. Feinberg, X. Chen, Y. J. Sun, R. Anil, and E. Hazan. Sketchy: Memory-efficient adaptive
regularization with frequent directions, 2023.
E. Frantar, E. Kurtic, and D. Alistarh. M-fac: Efficient matrix-free approximations of second-order
information, 2021.
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming, 2016. ISSN 156(1-2):59–99.
V. Gupta, T. Koren, and Y. Singer. Shampoo: Preconditioned stochastic tensor optimization. In
International Conference on Machine Learning, pages 1842–1850. PMLR, 2018.
M. He, Y. Liang, J. Liu, and D. Xu. Convergence of adam for non-convex objectives: Relaxed
hyperparameters and non-ergodic case. Journal of Machine Learning Research, 2023.
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding, 2021.
S. P. Karimireddy, Q. Rebjock, S. U. Stich, and M. Jaggi. Error feedback fixes SignSGD and other
gradient compression schemes. In Proceedings of the Thirty-sixth International Conference on
Machine Learning, pages 3252–3261, 2019.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014.
A. N. Lee, C. J. Hunter, and N. Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. arXiv
preprint arXiv:2308.07317, 2023.
X. Li, B. Karimi, and P. Li. On distributed adaptive optimization with gradient compression. arXiv
preprint arXiv:2205.05632, 2022.
H. Liu, Z. Li, D. Hall, P. Liang, and T. Ma. Sophia: A scalable stochastic second-order optimizer for
language model pre-training. arXiv preprint arXiv:2305.14342, 2023.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In Proceedings of the Seventh
International Conference on Learning Representations, 2019.
11

Y. Luo, X. Ren, Z. Zheng, Z. Jiang, X. Jiang, and Y. You. Came: Confidence-guided adaptive memory
efficient optimization. arXiv preprint arXiv:2307.02047, 2023.
I.-V. Modoranu, A. Kalinov, E. Kurtic, and D. Alistarh. Error feedback can accurately compress
preconditioners. arXiv preprint arXiv:2306.06098, 2023.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237, 2019.
P. Richtárik, I. Sokolov, and I. Fatkhullin. Ef21: A new, simpler, theoretically better, and practically
faster error feedback. Advances in Neural Information Processing Systems, 34:4384–4396, 2021.
K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. WINOGRANDE: an adversarial winograd
schema challenge at scale, 2019.
F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application
to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the
International Speech Communication Association, 2014.
N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In
International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.
R. Spring, A. Kyrillidis, V. Mohan, and A. Shrivastava. Compressing gradient optimizers via
count-sketches. In International Conference on Machine Learning, pages 5946–5955. PMLR,
2019.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient
foundation language models, 2023.
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Fun-
towicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,
M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 38–45, Online, Oct. 2020. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.
X. Xie, P. Zhou, H. Li, Z. Lin, and S. Yan. Adan: Adaptive nesterov momentum algorithm for faster
optimizing deep models, 2023.
Z. Yao, A. Gholami, S. Shen, K. Keutzer, and M. W. Mahoney. Adahessian: An adaptive second
order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020.
R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish
your sentence?, 2019.
S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
L. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore: Memory-efficient llm
training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024.
D. Zhou, J. Chen, Y. Cao, Z. Yang, and Q. Gu. On the convergence of adaptive gradient methods for
nonconvex optimization. Transactions on Machine Learning Research, 2024a. ISSN 2835-8856.
URL https://openreview.net/forum?id=Gh0cxhbz3c. Featured Certification.
P. Zhou, X. Xie, Z. Lin, and S. Yan. Towards understanding convergence and generalization of
adamw. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–8, 2024b. doi:
10.1109/TPAMI.2024.3382294.
12

Contents
1
Introduction
1
2
Related Work
2
3
The MICROADAM Algorithm
3
3.1
Efficient Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
3.2
Memory footprint analysis for the optimizer states and comparison with other methods
5
4
Convergence Guarantees for MICROADAM
6
4.1
Gradient and Error Compression . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
4.2
Convergence Guarantees for General Smooth Non-convex Functions . . . . . . . .
7
4.3
Convergence Rate for Non-Convex Functions under the PL Condition . . . . . . .
7
5
Experiments
8
6
Limitations and Broader Impact
10
A Additional Explanations and Experimental Details
14
B Training Settings and Hyper-parameters
14
B.1
GLUE/MNLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
B.2
GSM-8k.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
B.3
ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
C Training Graphs
15
D Memory footprint for the optimizer state
16
E
Deferred Proofs
18
E.1
Intermediate Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
E.2
Non-convex Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
E.3
Analysis Under PL Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
E.4
Non-convex Analysis with Weight Decay
. . . . . . . . . . . . . . . . . . . . . .
30
F
Error Feedback applied to GaLore
34
F.1
Behaviour of the Error Feedback Mechanism
. . . . . . . . . . . . . . . . . . . .
34
F.2
Consequences on Training
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
13

A
Additional Explanations and Experimental Details
Optimization set. The parameters included in the optimization set usually vary depending on the
model and optimizer type. For GLUE, we do not include the embeddings in the optimization set for
any of the optimizers because our experiments showed no significant difference when optimizing the
embeddings. Moreover, it is more fair for GaLore which would have an increased memory usage
due to the projection matrix for the embeddings. For LLaMa2 and ResNet models, we include all
layers in the optimization set, regardless of their type. This means that MICROADAM updates at most
10% of the weights in each layer. In the original work, GaLore was applied only to a subset of layers,
such as Q-, K-, V-, O-, up-, down- and gate-projection layers. Applying GaLore to all layers in the
same way as we do with MICROADAM would result in much larger memory usage because of the
projection matrices for the embeddings. Moreover, computing SVD for the embedding layer would
be infeasible. As a result, we omit the GaLore for LLaMa2 and ResNet experiments.
Implicit regularization. In our results so far, we observed MICROADAM having better performance
for a few LLM finetuning tasks, but especially for the ResNet/ImageNet results, where the difference
was statistically significant (around 1%). Our intuition for this behavior mainly comes from the
accuracy curves in Figure 6 and Figure 7. The trajectories of MICROADAM and SGD are typical for
regularized training. We hypothesize that MICROADAM has an implicit regularization mechanism
because the model update ut is 90% sparse, which leads to only updating 10% of the model
parameters at each step in each layer. In contrast to a 100% dense update ut, a sparse update
would not change the model parameters as much as a dense one. In the ResNet experiments, all
optimizers used the same regularization parameter λ = 1e −4, but the accuracy graph shows that
MICROADAM is more regularized than SGD, while the graphs for AdamW and AdamW-8bit look
like the regularization does not have any effect.
Discussion. In summary, the experimental results have shown that MICROADAM can recover
the state-of-the-art accuracy of the the uncompressed Adam baseline, while providing significant
memory gains and matching wall-clock speed on billion-parameter models. Specifically, our approach
matches and outperforms Adam-8b and CAME both in terms of memory use and in terms of final
accuracy. Relative to the high-compression GaLore method, MICROADAM provides consistently
higher accuracy, as well as more stable practical convergence. We conclude that MICROADAM should
be a good alternative to Adam-8bit in memory-constrained settings, and that the empirical results
appear to validate our theoretical predictions.
B
Training Settings and Hyper-parameters
In this section we provide details about the hyper-parameters that we used for each model and
dataset. We train all our models in bfloat16 format, tune the learning rates on a grid and report the
best accuracy among 3 seeds (7, 42 and 1234) and report the results for the best configuration that
converged.
All Adam variants use default parameters β1 = 0.9, β2 = 0.999, ϵ = 10−8 and the regularization
parameter λ is 0 for finetuning and 3e −4 for ImageNet pre-training. MICROADAM uses a window
size of m = 10 gradients with k = 1% density (equivalent to 99% sparsity and quantization bucket
size is set to 64 for the error feedback.
For GaLore we use rank r = 256 and the SVD update interval is set to T = 200, as suggested by the
original paper. We run our experiments on NVidia GPUs A100-SXM4-80GB, H100-80GB and on
RTX 3090 with 24GB RAM in single GPU setup.
B.1
GLUE/MNLI
For GLUE/MNLI, we used the learning rate grid {1e −6, 3e −6, 5e −6, 7e −6, 1e −5, 3e −5, 5e −
5, 7e −5} for all optimizers and models. Certain optimizers diverge for specific seeds. Next, we
provide some details about hyper-parameters for each optimizer individually.
MICROADAM. We use m = 10 gradients in the sliding window, k = 1% density (e.g. 99% sparsity)
and quantization bucket size 64 (we also tried 100 000, but this didn’t affect performance or memory
usage in a meaningful way).
14

Adam and Adam-8bit. All hyper-parameters mentioned above apply for these two main baseline
optimizers.
GaLore. We use rank r = 256 and SVD update interval T ∈{20, 200}. In the original GaLore
paper, the authors tune both learning rate and in our experiments we keep scale fixed to value 1 and
augment the learning rate grid with the values {1e −4, 3e −4, 5e −4, 7e −4}.
CAME. This optimizer has some additional parameters that we keep to default values, such as
β3 = 0.9999. Instead of ϵ, it uses ϵ1 = 1e −30 and ϵ2 = 1e −16. The authors mention that the
learning rate should be much smaller than Adam’s and because of that we augment the learning rate
grid with the values {1e −7, 3e −7, 5e −7, 7e −7}.
B.2
GSM-8k.
For GSM-8k, we used the learning rate grid {1e −5, 2e −5, 3e −5, 4e −5, 5e −5, 6e −5, 7e −
5, 8e −5, 9e −5} and reported the model with the best evaluation accuracy. We found that different
versions for PyTorch, lm-eval-harness and llm-foundry have large variance in the results.
MICROADAM. We use similar settings as for GLUE/MNLI above in terms of other hyper-parameters.
B.3
ImageNet
For ImageNet, we integrate our MICROADAM in the FFCV repository, which is highly tuned for
ResNets and SGD. We use E = 100 epochs, batch size 1024, cosine learning rate schedule with
warmup and image resolution 224 × 224 and precision bfloat16. We started from the initial learning
rate η = 1.024 tuned in the repository which scored highest accuracy for SGD. This learning rate
also worked well for MICROADAM, but it didn’t work for AdamW and AdamW-8bit. For these two
Adam variants we divided the learning rate by 2 until the models converged.
ResNet-18. For AdamW, the learning rate is η = 0.016 and for AdamW-8bit is η = 0.032.
ResNet-50. For AdamW, the learning rate is η = 0.008 and for AdamW-8bit is η = 0.008.
C
Training Graphs
In this section we show training loss curves for BERT-Base, BERT-Large and OPT-1.3b on
GLUE/MNLI and Llama-2 7B/13B on GSM-8k and ResNet-18/50 on ImageNet.
Figure 2: Training curves for BERT-Base on GLUE/MNLI
0
50
100
150
200
250
300
350
step
0.2
0.4
0.6
0.8
1.0
train/loss
Training Loss for BERT-Base
MicroAdam
AdamW-8bit
GaLore
AdamW
CAME
15

Figure 3: Training curves for BERT-Large on GLUE/MNLI
0
50
100
150
200
250
300
350
step
0.2
0.4
0.6
0.8
1.0
1.2
train/loss
Training Loss for BERT-Large
MicroAdam
AdamW-8bit
GaLore
AdamW
CAME
Figure 4: Training curves for OPT-1.3B on GLUE/MNLI
0
50
100
150
200
250
300
350
step
0.2
0.4
0.6
0.8
1.0
1.2
train/loss
Training Loss for OPT-1.3B
MicroAdam
AdamW-8bit
GaLore
AdamW
CAME
D
Memory footprint for the optimizer state
In this section we provide a python script to simulate the memory usage for our optimizer’s state for
Llama2-7b model. Note that the theoretical memory usage will always be slightly lower than the
actual allocated memory on the GPU because PyTorch usually allocates more. To run this script, run
the following commands:
import math
d = 6_738_415_616 #
a c t u a l
number
of
parameters
f o r
Llama−2 7b
k = math . c e i l ( d
/
100)
m = 10
M_AW32 = 8 * d
/
(2 ** 30)
M_AW16 = 4 * d
/
(2 ** 30)
M_AW8 = 2 * d
/
(2 ** 30)
M_muA = ( 0 . 5
* d + 4 * m * k )
/
(2 ** 30)
# B to GB
16

Figure 5: Training curves for Llama-2 7B on GSM-8k
0
100
200
300
400
500
600
700
step
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
loss/train/total
Training Loss for Llama-2 7B
MicroAdam
AdamW-8bit
AdamW
Figure 6: Pre-training for ResNet-18 on ImageNet
0
20
40
60
80
100
epoch
1
2
3
4
5
6
train loss
Training loss for ResNet-18/ImageNet
MicroAdam
AdamW-8bit
SGD
AdamW
0
20
40
60
80
100
epoch
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
top 1
Validation accuracy for ResNet-18/ImageNet
MicroAdam
AdamW-8bit
SGD
AdamW
Figure 7: Pre-training for ResNet-50 on ImageNet
0
20
40
60
80
100
epoch
1
2
3
4
5
6
train loss
Training loss for ResNet-50/ImageNet
MicroAdam
SGD
AdamW-8bit
AdamW
0
20
40
60
80
100
epoch
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
top 1
Validation accuracy for ResNet-50/ImageNet
MicroAdam
SGD
AdamW-8bit
AdamW
print ( f ' {M_AW32= : . 2 f } GB ' )
print ( f ' {M_AW16= : . 2 f } GB ' )
print ( f ' {M_AW8= : . 2 f } GB ' )
print ( f ' {M_muA= : . 2 f } GB ' )
GL_sumA = 1_423_872 # sum_Ai from Llama−2 7B
e p s i l o n = 266 _240 # sum of
s i z e s
f o r
rank −1 l a y e r s
for
b i t s ,
const
in
[ ( 8 ,
4) ,
(16 ,
6 ) ] :
for
rank
in
[256 ,
1024]:
dr = rank * GL_sumA
M_GLAW_rank = ( const * dr + 2 *
e p s i l o n )
/
(2 ** 30)
print ( f 'M_GLAW{ b i t s } _rank { rank }={M_GLAW_rank : . 2 f } GB ' )
17

E
Deferred Proofs
At time step t, let the uncompressed stochastic gradient be gt = e∇θf(θt), the error accumulator
be et, and the compressed gradient after the error correction be ˜gt = C(gt + et). The second
moment computed by the compressed gradients is denoted as vt = β2vt−1 + (1 −β2)˜g2
t , and
ˆvt = max{ˆvt−1, vt} is the AMSGrad normalization for the second-order momentum. Besides the
first-order gradient momentum mt used in the algorithm description, we define similar running
average sequence m′
t based on the uncompressed gradients gt.
mt = β1mt−1 + (1 −β1)˜gt
and
m′
t = β1m′
t−1 + (1 −β1)gt,
Note that m′
t is used only in the analysis, we do not need to store or compute it. By construction we
have
mt = (1 −β1)
t
X
τ=1
βt−τ
1
˜gτ,
m′
t = (1 −β1)
t
X
τ=1
βt−τ
1
gτ
Denote by ζt = et+1 −(et +gt −˜gt) = Q(et +gt −˜gt)−(et +gt −˜gt) the compression noise from
Q. Due to unbiasedness of the compressor Q (see Assumption 2), we have E[ζt | θt, gt, ˜gt, et] = 0.
Also, from the update rule of et+1 we get et+1 = et + gt −˜gt + ζt. Moreover, we use the following
auxiliary sequences,
Et+1 := β1Et + (1 −β1)et+1 = (1 −β1)
t+1
X
τ=1
βt+1−τ
1
eτ.
Zt+1 := β1Zt + (1 −β1)ζt+1 = (1 −β1)
t+1
X
τ=1
βt+1−τ
1
ζτ.
E.1
Intermediate Lemmas
Lemma 1. Consider Algorithm 2 with randomized rounding, i.e., for a vector x ∈Rd with δ =
mini xi and ∆= maxi xi, let ˆxi := ⌊xi−δ
u
+ ξ⌋u + δ be the i-th coordinate of the quantized vector
ˆx, where ξ ∼U[0, 1] is the uniform random variable and u = ∆−δ
2b−1 is the quantization level. Then
E[ˆx] = x,
∥ˆx −x∥≤
√d−2
2b−1
∆−δ
√
∆2+δ2 ∥x∥,
for all x ∈Rd.
Proof. The unbiasedness can be verified directly from the definition for each coordinate. Without
loss of generality assume that δ = x1 ≤x2 ≤· · · ≤xd−1 ≤xd = ∆. By construction of the
quantization, we have |ˆx1 −x1| = |ˆxd −xd| = 0 and |ˆxi −xi| ≤u for the remaining coordinates
2 ≤i ≤d −1. Then
∥ˆx −x∥2 =
d
X
i=1
|ˆxi −xi|2 ≤(d −2)u2 ≤(d −2)u2
∆2 + δ2 ∥x∥2,
which completes the proof.
Lemma 2. Under Assumptions 1-5, for all iterates t and T we have
∥m′
t∥≤G,
and
T
X
t=1
E[∥m′
t∥2] ≤Tσ2 +
T
X
t=1
E[∥∇f(θt)∥2].
Proof. The first part follows from triangle inequality and the Assumption 4 on bounded stochastic
gradient:
∥m′
t∥= (1 −β1)

t
X
τ=1
βt−τ
1
gτ
 ≤(1 −β1)
t
X
τ=1
βt−τ
1
∥gτ∥≤G.
For the second claim, the expected squared norm of average stochastic gradient can be bounded by
E

∥gt∥2
= E

∥gt −∇f(θt))∥2
+ E[∥∇f(θt)∥2] ≤σ2 + E[∥∇f(θt)∥2],
(1)
18

where we use Assumption 5 that gt is unbiased with bounded variance. Let gt,j denote the j-th
coordinate of gt. Applying Jensen’s inequality for the squared norm, we get
E[∥m′
t∥2]
=
E


(1 −β1)
t
X
τ=1
βt−τ
1
gτ

2

≤
(1 −β1)
t
X
τ=1
βt−τ
1
E[∥gτ∥2]
≤
σ2 + (1 −β1)
t
X
τ=1
βt−τ
1
E[∥∇f(θτ)∥2],
Summing over t = 1, . . . , T, we obtain
T
X
t=1
E[∥m′
t∥2] ≤Tσ2 + (1 −β1)
T
X
t=1
t
X
τ=1
βt−τ
1
E[∥∇f(θτ)∥2] ≤Tσ2 +
T
X
t=1
E[∥∇f(θt)∥2],
which completes the proof.
Lemma 3. Let qω = (1 + ω)q < 1. Under Assumptions 1-5, for all iterates t we have
∥et∥2 ≤
4q2
ω
(1 −q2ω)2 G2,
E[∥et+1∥2] ≤
4q2
ω
(1 −q2ω)2 σ2 +
2q2
ω
1 −q2ω
t
X
τ=1
1 + q2
ω
2
t−τ
E[∥∇f(θτ)∥2].
Proof. We start by using Assumption 1, 2 on compression and Young’s inequality to get
∥et+1∥2 = ∥Q(gt + et −C(gt + et))∥2
≤(1 + ω)2q2∥gt + et∥2
≤q2
ω(1 + ρ)∥et∥2 + q2
ω

1 + 1
ρ

∥gt∥2
≤1 + q2
ω
2
∥et∥2 +
2q2
ω
1 −q2ω
∥gt∥2,
(2)
where (2) is derived by choosing ρ =
1−q2
ω
2q2ω
and the fact that qω < 1. For the first claim we
recursively apply the obtained inequality and use bounded gradient Assumption 4. For the second
claim, initialization e1 = 0 and the obtained recursion imply
E[∥et+1∥2]
≤
2q2
ω
1 −q2ω
t
X
τ=1
1 + q2
ω
2
t−τ
E[∥gτ∥2]
(1)
≤
4q2
ω
(1 −q2ω)2 σ2 +
2q2
ω
1 −q2ω
t
X
τ=1
1 + q2
ω
2
t−τ
E[∥∇f(θτ)∥2],
which concludes the lemma.
Lemma 4. Let qω = (1 + ω)q < 1. Under Assumptions 1-5, for all iterates t we have
∥ζt∥≤ωq

1 +
2qω
1 −q2ω

G,
and
∥Zt∥≤ωq

1 +
2qω
1 −q2ω

G.
Proof. Using the bounds defining compressors and Lemma 3, we get
∥ζt∥= ∥Q(et + gt −˜gt) −(et + gt −˜gt)∥
≤ω∥et + gt −˜gt∥= ω∥et + gt −C(et + gt)∥
≤ωq∥et + gt∥
≤ωq∥et∥+ ωq∥gt∥
≤ωq

1 +
2qω
1 −q2ω

G.
19

For the second claim, recall the definition of Zt and apply triangle inequality:
∥Zt∥≤(1 −β1)
t
X
τ=1
βt−τ∥ζτ∥≤ωq

1 +
2qω
1 −q2ω

G.
Lemma 5. For the moving average error sequence Et, it holds that
T
X
t=1
E[∥Et∥2] ≤
4Tq2
ω
(1 −q2ω)2 σ2 +
4q2
ω
(1 −q2ω)2
T
X
t=1
E[∥∇f(θt)∥2].
Proof. Let et,j be the j-th coordinate of et and denote
Kt :=
t
X
τ=1

1+q2
ω
2
t−τ
E[∥∇f(θτ)∥2].
Applying Jensen’s inequality and Lemma 3, we get
E[∥Et∥2] = E


(1 −β1)
t
X
τ=1
βt−τ
1
eτ

2

≤(1 −β1)
t
X
τ=1
βt−τ
1
E[∥eτ∥2]
≤
4q2
ω
(1 −q2ω)2 σ2 + 2q2
ω(1 −β1)
(1 −q2ω)
t
X
τ=1
βt−τ
1
Kτ,
Summing over t = 1, . . . , T and using the technique of geometric series summation leads to
T
X
t=1
E[∥Et∥2] ≤
4Tq2
ω
(1 −q2ω)2 σ2 + 2q2
ω(1 −β1)
(1 −q2ω)
T
X
t=1
t
X
τ=1
βt−τ
1
Kτ
≤
4Tq2
ω
(1 −q2ω)2 σ2 +
2q2
ω
(1 −q2ω)
T
X
t=1
Kt
=
4Tq2
ω
(1 −q2ω)2 σ2 +
2q2
ω
(1 −q2ω)
T
X
t=1
t
X
τ=1
1 + q2
ω
2
t−τ
E[∥∇f(θτ)∥2]
≤
4Tq2
ω
(1 −q2ω)2 σ2 +
4q2
ω
(1 −q2ω)2
T
X
t=1
E[∥∇f(θt)∥2],
The desired result is obtained.
Lemma 6. Let qω = (1 + ω)q < 1. Under Assumptions 1-5, for all iterates t ∈[T] and coordinates
i ∈[d], the following bound holds
ˆvt,i ≤4(1 + q2
ω)3
(1 −q2ω)2 G2.
Proof. Lemma 3 and Assumption 4 imply
∥˜gt∥2 = ∥C(gt + et)∥2
≤∥C(gt + et) −(gt + et) + (gt + et)∥2
≤2(q2 + 1)∥gt + et∥2
≤4(q2 + 1)

G2 +
4q2
ω
(1 −q2ω)2 G2

= 4(1 + q2)(1 + q2
ω)2
(1 −q2ω)2
G2.
20

It’s then easy to show by the updating rule of ˆvt, there exists a j ∈[t] such that ˆvt,i = vj,i. Then
ˆvt,i = (1 −β2)
j
X
τ=1
βj−τ
2
˜g2
τ,i ≤4(1 + q2)(1 + q2
ω)2
(1 −q2ω)2
G2,
which concludes the claim.
Lemma 7. For Dt :=
1
√
ˆvt−1+ϵ −
1
√ˆvt+ϵ we have
T
X
t=1
∥Dt∥1 ≤d
√ϵ,
T
X
t=1
∥Dt∥2 ≤d
ϵ .
Proof. By the update rule, we have ˆvt−1,i ≤ˆvt,i for any iterate t and coordinate i ∈[d]. Therefore,
by the initialization ˆv0 = 0, we get
T
X
t=1
∥Dt∥1 =
T
X
t=1
d
X
i=1
 
1
p
ˆvt−1,i + ϵ −
1
p
ˆvt,i + ϵ
!
=
d
X
i=1
 
1
p
ˆv0,i + ϵ −
1
p
ˆvT,i + ϵ
!
≤d
√ϵ.
For the sum of squared l2 norms, note the fact that for a ≥b > 0, it holds that
(a −b)2 ≤(a −b)(a + b) = a2 −b2.
Thus,
T
X
t=1
∥Dt∥2 =
T
X
t=1
d
X
i=1
 
1
p
ˆvt−1,i + ϵ −
1
p
ˆvt,i + ϵ
!2
≤
T
X
t=1
d
X
i=1

1
ˆvt−1,i + ϵ −
1
ˆvt,i + ϵ

≤d
ϵ ,
which gives the desired result.
E.2
Non-convex Analysis
Here we derive the convergence rate with fixed step-size η. The rate shown in the main part can be
obtained by plugging the expression of η shown after the proof.
Theorem 3. (Non-convex convergence rate) Let Assumptions 1, 2, 3, 4, 5 hold and qω := (1+ω)q <
1. Then, choosing any step-size η ≤
ϵ
4LC0 , MICROADAM (Algorithm 3) satisfies
1
T
PT
t=1 E[∥∇f(θt)∥2] ≤2C0

f(θ1)−f ∗
T η
+ ηLσ2
ϵ
+ ηLC2
2G2
ϵ
+ η2L2C0C2
1G2
ϵ2
+ (1+C1)G2d
T √ϵ
+ η(1+2C1)C1LG2d
T ϵ

,
with constants C0 :=
q
4(1+q2ω)3
(1−q2ω)2 G2 + ϵ, C1 :=
β1
1−β1 (1 + C2) +
2qω
1−q2ω , C2 := ωq(1 +
2qω
1−q2ω ).
Proof. Similar to the proof of Comp-AMS [Li et al., 2022], we define two virtual iterates θ′
t and xt.
θ′
t+1 := θt+1 −η
Et+1
√ˆvt + ϵ
xt+1 := θ′
t+1 −η
β1
1 −β1
m′
t + Zt
√ˆvt + ϵ .
21

Then, we derive the recurrence relation for each sequence as follows:
θ′
t+1 = θt+1 −η
Et+1
√ˆvt + ϵ
= θt −η (1 −β1) Pt
τ=1 βt−τ
1
˜gτ + (1 −β1) Pt+1
τ=1 βt+1−τ
1
eτ
√ˆvt + ϵ
= θt −η (1 −β1) Pt
τ=1 βt−τ
1
(˜gτ + eτ+1) + (1 −β)βt
1e1
√ˆvt + ϵ
= θt −η (1 −β1) Pt
τ=1 βt−τ
1
(gτ + eτ + ζτ)
√ˆvt + ϵ
= θt −η (1 −β1) Pt
τ=1 βt−τ
1
eτ
√ˆvt + ϵ
−η
m′
t
√ˆvt + ϵ −η
Zt
√ˆvt + ϵ
= θt −η
Et
p
ˆvt−1 + ϵ
−η
m′
t
√ˆvt + ϵ + η
 
1
p
ˆvt−1 + ϵ
−
1
√ˆvt + ϵ
!
Et −η
Zt
√ˆvt + ϵ
= θ′
t −η
m′
t
√ˆvt + ϵ + η
 
1
p
ˆvt−1 + ϵ
−
1
√ˆvt + ϵ
!
Et −η
Zt
√ˆvt + ϵ
= θ′
t −η m′
t + Zt
√ˆvt + ϵ + ηDtEt,
where we used the fact that ˜gt + et+1 = gt + et + ζt with quantization noise ζt, and e0 = 0 at
initialization. Next, for the xt iterates we have
xt+1 = θ′
t+1 −η
β1
1 −β1
m′
t + Zt
√ˆvt + ϵ
= θ′
t −η m′
t + Zt
√ˆvt + ϵ −η
β1
1 −β1
m′
t + Zt
√ˆvt + ϵ + ηDtEt
= θ′
t −η
β1(m′
t−1 + Zt−1) + (1 −β1)(gt + ζt) +
β2
1
1−β1 (m′
t−1 + Zt−1) + β1(gt + ζt)
√ˆvt + ϵ
+ ηDtEt
= θ′
t −η
β1
1 −β1
m′
t−1 + Zt−1
√ˆvt + ϵ
−η gt + ζt
√ˆvt + ϵ + ηDtEt
= xt −η gt + ζt
√ˆvt + ϵ + η
β1
1 −β1
Dt(m′
t−1 + Zt−1) + ηDtEt.
Next we apply smoothness (Assumption 3) of the loss function f over the iterates xt. From the
gradient Lipschitzness we have
f(xt+1) ≤f(xt) + ⟨∇f(xt), xt+1 −xt⟩+ L
2 ∥xt+1 −xt∥2.
22

Due to unbiasedness of the compressor Q (see Assumption 2), we have E[ζt|gt, ˜gt, et, ˆvt] = 0.
Taking expectation, we obtain
E[f(xt+1)] −E[f(xt)]
≤
−ηE

∇f(xt), gt + ζt
√ˆvt + ϵ

+ηE

∇f(xt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

+η2L
2 E
"
gt + ζt
√ˆvt + ϵ −
β1
1 −β1
Dt(m′
t−1 + Zt−1) −DtEt

2#
=
−ηE

∇f(θt),
gt
√ˆvt + ϵ

|
{z
}
I
(3)
+ ηE

∇f(xt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

|
{z
}
II
+ η2L
2 E
"
gt + ζt
√ˆvt + ϵ −
β1
1 −β1
Dt(m′
t−1 + Zt−1) −DtEt

2#
|
{z
}
III
+ ηE

∇f(θt) −∇f(xt),
gt
√ˆvt + ϵ

|
{z
}
IV
,
(4)
In the following, we bound all the four terms highlighted above.
Bounding term I. We have
I
=
−ηE
"*
∇f(θt),
gt
p
ˆvt−1 + ϵ
+#
−ηE
"*
∇f(θt),
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!
gt
+#
≤
−ηE
"*
∇f(θt),
∇f(θt)
p
ˆvt−1 + ϵ
+#
+ ηG2E[∥Dt∥].
≤
−
η
q
4(1+q2
ω)3
(1−q2ω)2 G2 + ϵ
E[∥∇f(θt)∥2] + ηG2E[∥Dt∥1],
(5)
where we use Assumption 4, Lemma 6 and the fact that l2 norm is no larger than l1 norm.
Bounding term II. By the definition of Et and Zt, we know that
∥Et∥
≤
(1 −β1)
t
X
τ=1
βt−τ
1
∥et∥≤
2qω
1 −q2ω
G,
∥Zt∥
≤
(1 −β1)
t
X
τ=1
βt−τ
1
∥ζt∥≤ωq

1 +
2qω
1 −q2ω

G.
23

Then we have
II ≤ηE

∇f(θt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

+ ηE

∇f(xt) −∇f(θt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

≤ηE

∥∇f(θt)∥

β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt


+ η2LE
"
β1
1−β1 m′
t−1 +
β1
1−β1 Zt−1 + Et
p
ˆvt−1 + ϵ


β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

#
≤ηC1G2E[∥Dt∥1] + η2C2
1LG2
√ϵ
E[∥Dt∥1],
(6)
where C1 =
β1
1−β1

1 + ωq

1 +
2qω
1−q2
ω

+
2qω
1−q2
ω . The second inequality is because of smoothness
of f(θ), and the last inequality is due to Lemma 3, Assumption 4 and the property of norms.
Bounding term III. This term can be bounded as follows:
III ≤η2LE
"
gt + ζt
√ˆvt + ϵ

2#
+ η2LE
"
β1
1 −β1
Dt(m′
t−1 + Zt−1) −DtEt

2#
≤2η2L
ϵ
E[∥gt −∇f(θt) + ∇f(θt)∥2] + 2η2L
ϵ
E[∥ζt∥2]
(7)
+ η2LE
"Dt

β1
1 −β1
m′
t−1 +
β1
1 −β1
Zt−1 −Et

2#
≤2η2L
ϵ
E[∥∇f(θt)∥2] + 2η2Lσ2
ϵ
+ 2η2L
ϵ
ω2q2

1 +
2q
1 −q2
2
G2 + η2C2
1LG2E[∥Dt∥2]
≤2η2L
ϵ
E[∥∇f(θt)∥2] + 2η2L(σ2 + C2
2G2)
ϵ
+ η2C2
1LG2E[∥Dt∥2],
(8)
where C2 = ωq(1 +
2q
1−q2 ) and we used Assumption 5 that gt is unbiased with bounded variance σ2.
Bounding term IV. We have
IV = ηE
"*
∇f(θt) −∇f(xt),
gt
p
ˆvt−1 + ϵ
+#
(9)
+ ηE
"*
∇f(θt) −∇f(xt),
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!
gt
+#
≤ηE
"*
∇f(θt) −∇f(xt),
∇f(θt)
p
ˆvt−1 + ϵ
+#
(10)
+ η2LE
"
β1
1−β1 m′
t−1 +
β1
1−β1 Zt−1 + Et
p
ˆvt−1 + ϵ
 ∥Dtgt∥
#
(a)
≤ηρ
2ϵ E[∥∇f(θt)∥2] + η
2ρE[∥∇f(θt) −∇f(xt)∥2] + η2C1LG2
√ϵ
E[∥Dt∥]
(b)
≤ηρ
2ϵ E[∥∇f(θt)∥2] + η3L2
2ρ E



β1
1−β1 m′
t−1 +
β1
1−β1 Zt−1 + Et
p
ˆvt−1 + ϵ

2
+ η2C1LG2
√ϵ
E[∥Dt∥1]
≤ηρ
2ϵ E[∥∇f(θt)∥2] + η3L2
2ρ
C2
1G2
ϵ
+ η2LC1G2
√ϵ
E[∥Dt∥1],
(11)
where (a) is due to Young’s inequality and (b) is based on Assumption 3. Now integrating (5), (6),
(8), (11) into (4),
24

I
≤
−η
C0
E[∥∇f(θt)∥2] + ηG2E[∥Dt∥1]
II
≤
ηC1G2E[∥Dt∥1] + η2C2
1LG2
√ϵ
E[∥Dt∥1]
III
≤
η2L
ϵ E[∥∇f(θt)∥2] + η2L(σ2 + C2
2G2)
ϵ
+ η2C2
1LG2E[∥Dt∥2]
IV
≤
ηρ
2ϵ E[∥∇f(θt)∥2] + η3L2
2ρ
C2
1G2
ϵ
+ η2LC1G2
√ϵ
E[∥Dt∥1],
and taking the telescoping summation over t = 1, . . . , T, we obtain
E[f(xT +1) −f(x1)]
≤

−η
C0
+ η2L
ϵ
+ ηρ
2ϵ

T
X
t=1
E[∥∇f(θt)∥2] + Tη2L(σ2 + C2
2G2)
ϵ
+ Tη3L2C2
1G2
2ρϵ
+

η(1 + C1)G2 + η2(1 + C1)C1LG2
√ϵ

T
X
t=1
E[∥Dt∥1] + η2C2
1LG2
T
X
t=1
E[∥Dt∥2].
Setting η ≤
ϵ
4LC0 and choosing ρ =
ϵ
2C0 , we further arrive at
E[f(xT +1) −f(x1)] ≤−η
2C0
T
X
t=1
E[∥∇f(θt)∥2] + Tη2L(σ2 + C2
2G2)
ϵ
+ Tη3L2C0C2
1G2
ϵ2
+ η(1 + C1)G2d
√ϵ
+ η2(1 + 2C1)C1LG2d
ϵ
.
where the inequality follows from Lemma 7. Re-arranging terms, we get that
1
T
T
X
t=1
E[∥∇f(θt)∥2]
≤2C0
E[f(x1) −f(xT +1)]
Tη
+ ηL(σ2 + C2
2G2)
ϵ
+ η2L2C0C2
1G2
ϵ2

+ 2C0
(1 + C1)G2d
T√ϵ
+ η(1 + 2C1)C1LG2d
Tϵ

≤2C0
f(θ1) −f ∗
Tη
+ ηL(σ2 + C2
2G2)
ϵ
+ η2L2C0C2
1G2
ϵ2

+ 2C0
(1 + C1)G2d
T√ϵ
+ η(1 + 2C1)C1LG2d
Tϵ

,
where in the last inequality we used x1 = θ1 and the lower bound f ∗≤f(θ) for all θ ∈Rd.
25

To get the rate mentioned in the main part, choose η = min{
ϵ
4LC0 ,
1
√
T } and continue
1
T
T
X
t=1
E[∥∇f(θt)∥2]
≤2C0

max

1, 4LC0
ϵ
√
T
 f(θ1) −f ∗
√
T
+ L(σ2 + C2
2G2)
ϵ
√
T
+ L2C0C2
1G2
ϵ2T

+ 2C0
(1 + C1)G2d
T√ϵ
+ (1 + 2C1)C1LG2d
ϵT 3/2

≤2C0
f(θ1) −f ∗
√
T
+ L(σ2 + C2
2G2)
ϵ
√
T

+ 2C0
4LC0
ϵ
f(θ1) −f ∗
T
+ L2C0C2
1G2
ϵ2T
+ (1 + C1)G2d
T√ϵ
+ (1 + 2C1)C1LG2d
ϵT 3/2

= 2C0
f(θ1) −f ∗
√
T
+ L(σ2 + C2
2G2)
ϵ
√
T

+ O
G3(G + d)
T

,
where in the second part of the rate we suppressed all the problem and compression dependent
constants.
E.3
Analysis Under PL Condition
As in the non-convex analysis, here we derive the convergence rate with fixed step-size η. The rate
shown in the main part can be obtained by plugging the expression of η.
Theorem 4. (Convergence rate under PL) Let Assumptions 1, 2, 3, 4, 5 and 6 hold, and qω < 1.
Then, choosing any step-size η ≤
ϵ
4LC0 , MICROADAM (Algorithm 3) satisfies
E[f(θT +1)] −f ∗≤
 1 −ηµ
C0
T (f(θ1) −f ∗) + η

LC0σ2+LC0(C1+C2
2)G2
µϵ
+ (1+C1)G2d+C1G2
√ϵ

+ η2 
3L2C0C2
1G2
2µϵ3/2
+ (1+2C1)C1LG2d
ϵ
+ LC2
1G2
2ϵ

.
Proof. We start from descent lemma
E[f(xt+1)] −f(xt)
≤−ηE

∇f(xt), gt + ζt
√ˆvt + ϵ

+ ηE

∇f(xt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

+ η2L
2 E
"
gt + ζt
√ˆvt + ϵ −
β1
1 −β1
Dt(m′
t−1 + Zt−1) −DtEt

2#
= −ηE

∇f(xt),
gt
√ˆvt + ϵ

|
{z
}
I′
+ ηE

∇f(xt),
β1
1 −β1
Dt(m′
t−1 + Zt−1) + DtEt

|
{z
}
II
+ η2L
2 E
"
gt + ζt
√ˆvt + ϵ −
β1
1 −β1
Dt(m′
t−1 + Zt−1) −DtEt

2#
|
{z
}
III
. (12)
26

We bound part II and part III in the same way as it was done in the non-convex analysis. We now
provide a bound for part I′:
I′ = −ηE

∇f(xt),
gt
√ˆvt + ϵ

= −ηE
"*
∇f(xt),
gt
p
ˆvt−1 + ϵ
+#
−ηE
"*
∇f(xt), gt
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!+#
= −ηE
"*
∇f(xt), gt −∇f(xt) + ∇f(xt)
p
ˆvt−1 + ϵ
+#
−ηE
"*
∇f(xt), gt
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!+#
= −ηE
"*
∇f(xt),
∇f(xt)
p
ˆvt−1 + ϵ
+#
−ηE
"*
∇f(xt), gt −∇f(xt)
p
ˆvt−1 + ϵ
+#
−ηE
"*
∇f(xt), gt
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!+#
.
We further expand and bound this equation as follows:
I′ ≤−η
C0
E
h
∥∇f(xt)∥2i
−ηE
"*
∇f(xt) −∇f(θt) + ∇f(θt),
1
p
ˆvt−1 + ϵ
(∇f(θt) −∇f(xt))
+#
−ηE
"*
∇f(xt) −∇f(θt) + ∇f(θt),
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!
gt
+#
= −η
C0
E
h
∥∇f(xt)∥2i
−ηE
"*
∇f(xt) −∇f(θt),
1
p
ˆvt−1 + ϵ
(∇f(θt) −∇f(xt))
+#
−ηE
"*
∇f(θt),
1
p
ˆvt−1 + ϵ
(∇f(θt) −∇f(xt))
+#
−ηE
"*
∇f(xt) −∇f(θt),
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!
gt
+#
−ηE
"*
∇f(θt),
 
1
√ˆvt + ϵ −
1
p
ˆvt−1 + ϵ
!
gt
+#
≤−η
C0
E
h
∥∇f(xt)∥2i
+ η
√ϵE
h
∥∇f(xt) −f(θt)∥2i
−ηE
"*
∇f(θt),
1
p
ˆvt−1 + ϵ
(∇f(θt) −∇f(xt))
+#
+ ηE [⟨∇f(xt) −∇f(θt), Dtgt⟩] + ηE [⟨∇f(θt), Dtgt⟩] .
Next, we use the Cauchy–Schwartz inequality to bound inner products above, L-smoothness inequality
to bound ∥∇f(xt) −∇f(θt)∥≤L∥xt −θt∥≤ηLC1G
√ϵ
, and the inequality −∥a∥2 ≤−1
2∥b∥2 +
27

∥a −b∥2 for the first term:
I′ ≤−η
C0
E
h
∥∇f(xt)∥2i
+ η
√ϵE
h
∥∇f(xt) −f(θt)∥2i
+ ηG
√ϵ E [∥∇f(θt) −∇f(xt)∥]
+ ηGE [∥∇f(xt) −∇f(θt)∥∥Dt∥] + ηG2E [∥Dt∥]
≤−
η
2C0
E
h
∥∇f(xt)∥2i
−
η
2C0
E
h
∥∇f(xt)∥2i
+ η
√ϵ
η2L2C2
1G2
ϵ
+ η2LC1G2
ϵ
+ ηGηLC1G
√ϵ
E [∥Dt∥1] + ηG2E [∥Dt∥1]
≤−
η
2C0
E
h
∥∇f(xt)∥2i
−
η
4C0
E[∥∇f(θt)∥2] +
η
2C0
E[∥∇f(xt) −∇f(θt)∥2]
+ η3L2C2
1G2
ϵ3/2
+ η2LC1G2
ϵ
+ η2LC1G2
√ϵ
E [∥Dt∥1] + ηG2E [∥Dt∥1]
≤−
η
2C0
E
h
∥∇f(xt)∥2i
−
η
4C0
E[∥∇f(θt)∥2]
+ 3η3L2C2
1G2
2ϵ3/2
+ η2LC1G2
ϵ
+ η2LC1G2
√ϵ
E [∥Dt∥1] + ηG2E [∥Dt∥1] .
Plugging the obtained bound for I′ with previously obtained bounds for II and III
II
≤
ηC1G2E[∥Dt∥1] + η2C2
1LG2
√ϵ
E[∥Dt∥1]
III
≤
η2L
ϵ E[∥∇f(θt)∥2] + η2L(σ2 + C2
2G2)
ϵ
+ η2C2
1LG2E[∥Dt∥2]
into (12) and using the step-size bound η ≤
ϵ
4LC0 we get
E[f(xt+1)] −E[f(xt)] ≤−
η
2C0
E
h
∥∇f(xt)∥2i
−
η
4C0
E[∥∇f(θt)∥2]
+ 3η3L2C2
1G2
2ϵ3/2
+ η2LC1G2
ϵ
+ η2LC1G2
√ϵ
E [∥Dt∥1]
+ ηC1G2E[∥Dt∥1] + η2C2
1LG2
√ϵ
E[∥Dt∥1] + ηG2E [∥Dt∥1]
+ η2L
ϵ E[∥∇f(θt)∥2] + η2L(σ2 + C2
2G2)
ϵ
+ η2C2
1LG2E[∥Dt∥2]
≤−
η
2C0
E
h
∥∇f(xt)∥2i
+ η2Lσ2
ϵ
+ η2L(C1 + C2
2)G2
ϵ
+ η(1 + C1)G2E[∥Dt∥1] + 3η3L2C2
1G2
2ϵ3/2
+ η2(1 + C1)C1LG2
√ϵ
E[∥Dt∥1] + η2C2
1LG2E[∥Dt∥2]
≤−ηµ
C0
(E[f(xt)] −f ∗) + η2Lσ2
ϵ
+ η2L(C1 + C2
2)G2
ϵ
+ η(1 + C1)G2E[∥Dt∥1] + η2(1 + C1)C1LG2
√ϵ
E[∥Dt∥1]
+ η2C2
1LG2E[∥Dt∥2] + 3η3L2C2
1G2
2ϵ3/2
,
28

where in the last inequality we applied PL condition from Assumption 6. After some reshuffling of
the terms, we obtain the following recursion:
E[f(xt+1)] −f ∗≤

1 −ηµ
C0

(E[f(xt)] −f ∗) + η2Lσ2
ϵ
+ η2L(C1 + C2
2)G2
ϵ
+ 3η3L2C2
1G2
2ϵ3/2
+ η(1 + C1)G2E[∥Dt∥1] + η2(1 + C1)C1LG2
√ϵ
E[∥Dt∥1]
+ η2C2
1LG2E[∥Dt∥2].
Notice that η ≤
ϵ
4LC0 ≤C0
4µ , so that the coefficient 1 −ηµ
C0 ∈(0, 1). Unrolling the recursion, we
arrive
E[f(xT +1)] −f ∗≤

1 −ηµ
C0
T
(E[f(x1)] −f ∗)
+
η2Lσ2
ϵ
+ η2L(C1 + C2
2)G2
ϵ
+ 3η3L2C2
1G2
2ϵ3/2

T
X
t=1

1 −ηµ
C0
t
+ η(1 + C1)G2
T
X
t=1

1 −ηµ
C0
t
E[∥Dt∥1]
+ η2(1 + C1)C1LG2
√ϵ
T
X
t=1

1 −ηµ
C0
t
E[∥Dt∥1]
+ η2C2
1LG2
T
X
t=1

1 −ηµ
C0
t
E[∥Dt∥2].
(13)
For the second sum above we upper bound it by its infinite sum as
T
X
t=1

1 −ηµ
C0
t
≤
∞
X
t=0

1 −ηµ
C0
t
= C0
ηµ.
For the other three sums we bound 1 −ηµ
C0 ≤1 and apply the bounds in Lemma 7:
T
X
t=1

1 −ηµ
C0
t
E[∥Dt∥1] ≤
T
X
t=1
E[∥Dt∥1] ≤d
√ϵ,
T
X
t=1

1 −ηµ
C0
t
E[∥Dt∥2] ≤
T
X
t=1
E[∥Dt∥2] ≤d
ϵ .
Plugging all this bounds into (13) and noticing that x1 = θ1, we finally get
E[f(xT +1)] −f ∗≤

1 −ηµ
C0
T
(f(θ1) −f ∗)
+ C0
ηµ
η2Lσ2
ϵ
+ η2L(C1 + C2
2)G2
ϵ
+ 3η3L2C2
1G2
2ϵ3/2

+ η(1 + C1)G2d
√ϵ
+ η2(1 + C1)C1LG2d
ϵ
+ η2C2
1LG2d
ϵ
≤

1 −ηµ
C0
T
(f(θ1) −f ∗)
+ η
LC0σ2
µϵ
+ LC0(C1 + C2
2)G2
µϵ
+ (1 + C1)G2d
√ϵ

+ η2
3L2C0C2
1G2
2µϵ3/2
+ (1 + C1)C1LG2d
ϵ
+ C2
1LG2d
ϵ

.
29

The obtained rate above is with respect to the virtual iterates xt that we defined for the purposes of
analysis. To convert this rate with respect to the iterates θt of the algorithm, we apply L-smoothness
to bound the functional difference:
|f(xt) −f(θt)| ≤|⟨∇f(θt), xt −θt)⟩| + L
2 ∥xt −θt∥2 ≤ηC1G2
√ϵ
+ η2LC2
1G2
2ϵ
,
which implies
E[f(θT +1)] −f ∗≤

1 −ηµ
C0
T
(f(θ1) −f ∗)
+ η
LC0σ2
µϵ
+ LC0(C1 + C2
2)G2
µϵ
+ (1 + C1)G2d
√ϵ
+ C1G2
√ϵ

+ η2
3L2C0C2
1G2
2µϵ3/2
+ (1 + C1)C1LG2d
ϵ
+ C2
1LG2d
ϵ
+ LC2
1G2
2ϵ

and completes the proof.
To get the rate mentioned in the main part of the paper, we plug in the expression η =
min{
ϵ
4LC0 , 2C0 log T
µT
} and collect higher order terms.
E[f(θT +1)] −f ∗≤max
 1
T 2 ,

1 −ϵµ
4L
T 
(f(θ1) −f ∗)
+ log T
T
2C0
µ
LC0σ2
µϵ
+ LC0(C1 + C2
2)G2
µϵ
+ (1 + C1)G2d
√ϵ
+ C1G2
√ϵ

+ log2 T
T 2
4C2
0
µ2
3L2C0C2
1G2
2µϵ3/2
+ (1 + C1)C1LG2d
ϵ
+ C2
1LG2d
ϵ
+ LC2
1G2
2ϵ

≤2 log T
T
LC2
0
µ
σ2 + (C1 + C2
2)G2
µϵ
+ C0(1 + C1)(1 + d)G2
µ√ϵ

+ e
O
G4(G + d)
T 2

.
E.4
Non-convex Analysis with Weight Decay
Algorithm 4 MICROADAMW (MICROADAM with Weight Decay)
1: Input: parameters β1, β2 ∈(0, 1), ϵ > 0, step-size η > 0, θ1 ∈Rd, e1 = m0 = v0 = ˆv0 = 0d
2: for t = {1, 2, ..., T} do
3:
gt = e∇θf(θt)
⋄Compute unbiased stochastic gradient
4:
˜gt = C(gt + et)
⋄Add accumulated error et and compress
5:
et+1 = Q(et + gt −˜gt)
⋄Update and compress the error
6:
mt = β1mt−1 + (1 −β1)˜gt
⋄Update first-order gradient moment
7:
vt = β2vt−1 + (1 −β2)˜g2
t
⋄Update second-order gradient moment
8:
θt+1 = (1 −ηtλ)θt −η
mt
√vt+ϵ
⋄Update the model parameters with weight decay
9: end for
Lemma 8. Under Assumptions 1-5, for all iterates of Algorithm 4 we have
E

∥mt −∇f(θt)∥2
≤

1 −β1
2

E

∥mt−1 −∇f(θt−1)∥2
+ 2
β1
L2E

∥θt −θt−1∥2
+ β1E∥∇f(θt) −˜gt∥2.
30

Proof. We start our proof from
E

∥mt −∇f(θt)∥2
=E

∥(1 −β1)mt−1 + β1˜gt −∇f(θt)∥2
=E [∥(1 −β1)mt−1 + (1 −β1)∇f(θt−1)
−(1 −β1)∇f(θt−1) + β1˜gt −∇f(θt)∥2
=E [∥(1 −β1)mt−1 + (1 −β1)∇f(θt−1) −(1 −β1)∇f(θt)
−(1 −β1)∇f(θt−1) + β1˜gt −β1∇f(θt)∥2
.
Using Jensen’s inequality we have
E

∥mt −∇f(θt)∥2
≤(1 −β1)E

∥mt−1 −∇f(θt−1) + ∇f(θt−1) −∇f(θt)∥2
+ β1E

∥˜gt −∇f(θt)∥2
.
Using Young’s inequality we have
E

∥mt −∇f(θt)∥2
≤(1 −β1)(1 + b)E

∥mt−1 −∇f(θt−1)∥2
+ (1 −β1)

1 + 1
b

E∥∇f(θt−1) −f(θt)∥2
+ β1E ∥∇f(θt) −˜gt∥2 .
Setting b = β1
2 we have (1 −β1)

1 + β1
2

≤1 −β1
2 and (1 −β1)

1 +
2
β1

≤
2
β1 :
E

∥mt −∇f(θt)∥2
≤

1 −β1
2

E

∥mt−1 −∇f(θt−1)∥2
+ 2
β1
E∥∇f(θt−1) −f(θt)∥2 + β1E ∥∇f(θt) −˜gt∥2 .
Combining this bound with L-smoothness we obtain
E

∥mt −∇f(θt)∥2
≤

1 −β1
2

E

∥mt−1 −∇f(θt−1)∥2
+ 2
β1
L2E∥θt−1 −θt∥2 + β1E ∥∇f(θt) −˜gt∥2 .
Lemma 9. Under Assumptions 1-5, for all iterates of Algorithm 4 we have
E

∥∇f(θt) −˜gt∥2
≤9E

∥et∥2
+ 6G2 + 3σ2.
Proof. We start from
E

∥∇f(θt) −˜gt∥2
=E

∥C(et + gt) −∇f(θt)∥2
=E

∥C(et + gt) −(et + gt) + (et + gt) −∇f(θt)∥2
≤3E

∥C(et + gt) −(et + gt)∥2
+ 3E

∥et∥2
+ 3E

∥∇f(θt) −gt∥2
.
Using definition of contractive compressor we have
E

∥∇f(θt) −˜gt∥2
≤3q2E

∥et + gt∥2
+ 3E

∥et∥2
+ 3E

∥∇f(θt) −gt∥2
.
Using Young’s inequality we have
E

∥∇f(θt) −˜gt∥2
≤6q2E

∥et∥2
+ 6q2E

∥gt∥2
+ 3E

∥et∥2
+ 3E

∥∇f(θt) −gt∥2
≤9E

∥et∥2
+ 6G2 + 3σ2.
31

Lemma 10. Under Assumptions 1-5, for all iterates of Algorithm 4 we have
E

∥et∥2
≤
(1 + ω)2q2
(1 −(1 + ω)q)2 G2
Proof. Using definition of contractive compressor we have
E

∥et+1∥2
= E
h
∥Q (et + gt −C(et + gt))∥2i
≤(1 + ω)2q2E

∥et + gt −C(et + gt)∥2
.
Using Young’s inequality we have
E

∥et+1∥2
≤(1 + ω)2q2 (1 + a) E

∥et∥2
+ (1 + ω)2q2

1 + 1
a

E

∥gt∥2
.
We need to satisfy the following condition:
(1 + ω)2q2(1 + a) ≤(1 + ω)q.
It holds for 0 ≤ω, 0 ≤q < 1, (1 + ω)q < 1 and a =
1
(1+ω)q −1. Using this parameters we have
E

∥et+1∥2
≤(1 + ω)qE

∥et∥2
+ (1 + ω)2q2

1
1 −(1 + ω)q

∥gt∥2
≤(1 + ω)qE

∥et∥2
+ (1 + ω)2q2
1 −(1 + ω)q E

∥gt∥2
.
Unrolling this recursion allows us to obtain
E

∥et+1∥2
≤((1 + ω)q)t E

∥e1∥2
+
T
X
t=1
((1 + ω)q)t (1 + ω)2q2
1 −(1 + ω)q G2
≤((1 + ω)q)t E

∥e1∥2
+
1
1 −(1 + ω)q
(1 + ω)2q2
1 −(1 + ω)q G2
≤
(1 + ω)2q2
(1 −(1 + ω)q)2 G2,
last inequality holds because e1 = 0.
Lemma 11. Under Assumptions 1-5, for all iterates of Algorithm 4 we have
E
h
∥˜gt∥2i
≤4(1 + q2)

1 +
(1 + ω)2q2
(1 −(1 + ω)q)2

G2.
Proof.
E
h
∥˜gt∥2i
= E
h
∥C (gt + et)∥2i
= E
h
∥C (gt + et) −(gt + et) + (gt + et)∥2i
≤2E
h
∥C (gt + et) −(gt + et)∥2i
+ 2E
h
∥gt + et∥2i
≤2(1 + q2)E
h
∥gt + et∥2i
.
Using Lemma 10 we obtain
E
h
∥˜gt∥2i
≤4(1 + q2)

1 +
(1 + ω)2q2
(1 −(1 + ω)q)2

G2.
32

Lemma 12 (From paper: Zhou et al. [2024b]). Let us consider the update rule:
θt+1 = (1 −ηλ)θt −ηt
mt
√vt + ε.
For brevity, we denote bvt = √vt + ε. Also we define
ut := mt + λθt ⊗bvt,
where ⊗denotes element-wise product. Moreover, we also define ef (θt) as follows:
ef (θt) = f(θt) + λt∥θt∥2
bvt
where λt = λ
2
Pt
i=1
  1−q
2
i for t > 0, λ0 = 0 with 0 < q < 1 and ∥θt∥ˆvt =
p
⟨θt, ˆvt ⊗θt⟩. Also
let c1 ≤∥bvt∥∞< c2, then iterates of Algorithm 4 satisfy
ef (θt) ≤ef (θt−1) + ηt
2c1
∥∇f (θt−1) −mt−1∥2
−ηt
2c2
∇ef (θt−1)

2
−ηt
4c2
∥ut−1∥2 .
Lemma 13 (From paper: Zhou et al. [2024b]). Assume that cs,∞≤∥˜gt∥∞≤c∞, then we have
∥mt∥∞≤c∞,
∥vt + ϵ∥∞c2
∞+ ϵ,

(vt + ϵ)p
(vt+1 + ϵ)p

∞
∈[1 −µ, 1 + µ] (∀p ∈(0, 1)) ,
where µ =
β2c2
∞
c2s,∞+ϵ.
Theorem 5. Let Assumptions 1 to 5 hold. Define Ψt = ef(θt) +
ηt
2c1β1 E

∥mt −∇f(θt)∥2
. With
ηt = η ≤β1c1
2L
q
c1
2c2 , Algorithm 4 satisfies
1
T
T
X
t=1
∇ef (θt−1)

2
≤2c2
ηT Ψ0
+ c2
c1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
Proof. We start from main lemma and lemma for momentum, summing inequalities together we
obtain
ef (θt) + V E

∥mt −∇f(θt)∥2
≤ef (θt−1) + ηt
2c1
∥∇f (θt−1) −mt−1∥2
−ηt
2c2
∇ef (θt−1)

2
−ηt
4c2
∥ut−1∥2
+ V

1 −β1
2

E

∥mt−1 −∇f(θt−1)∥2
+ V 2
β1
L2E∥θt−1 −θt∥2 + V β1E ∥∇f(θt) −˜gt∥2 .
Using previous lemmas we have
ef (θt) + V E

∥mt −∇f(θt)∥2
≤ef (θt−1) + ηt
2c1
∥∇f (θt−1) −mt−1∥2
−ηt
2c2
∇ef (θt−1)

2
−ηt
4c2
∥ut−1∥2
+ V

1 −β1
2

E

∥mt−1 −∇f(θt−1)∥2
+ V 2
β1
L2E∥θt−1 −θt∥2
+ V β1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
33

Using θt −θt−1 = −ηt
ut−1
bvt−1 we have
ef (θt) + V E

∥mt −∇f(θt)∥2
≤ef (θt−1) −
 ηt
4c2
−2V L2η2
t
β1c2
1

∥ut−1∥2
−ηt
2c2
∇ef (θt−1)

2
+

V

1 −β1
2

+ ηt
2c1

E

∥mt−1 −∇f(θt−1)∥2
+ V β1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
Using V =
ηt
2c1β1 and Ψt = ef(θt) +
ηt
2c1β1 E

∥mt −∇f(θt)∥2
we have
Ψt ≤Ψt−1 −ηt
4c2

1 −4c2L2η2
t
β2
1c3
1

∥ut−1∥2
−ηt
2c2
∇ef (θt−1)

2
+ ηt
2c1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
Using ηt = η ≤β1c1
2L
q
c1
2c2 we have
Ψt ≤Ψt−1 −ηt
2c2
∇ef (θt−1)

2
+ η
2c1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
Summing from t = 1 to T we have
1
T
T
X
t=1
∇ef (θt−1)

2
≤2c2
ηT Ψ0 + c2
c1
 
9
 
1 +
(1 + ω)2q2
(1 −(1 + ω)q)2
!
G2 + 6G2 + 3σ2
!
.
Discussion. This result is similar to the one from Zhou et al. [2024b] in the non-convex case, where
the decay rate for the first term is O
  1
T

and the second term is a non-vanishing O

β1 c2
c1 σ2
. In
our result, the non-vanishing term is proportional to O

c2
c1
 σ2 + G2
.
A key difference is that our term is not proportional to β1. It is important to note that β1 typically
takes a value close to 1 in practical applications, meaning its influence on the bound is minimal.
Therefore, even though our result does not directly involve β1, the impact on the overall bound is not
significantly different.
Moreover, our bound includes an additional term proportional to G2, which represents the gradient
norm squared. This makes the bound slightly worse compared to the result in Zhou et al. [2024b].
However, this degradation is only by a constant factor, which means that while the theoretical bound
may be worse, the practical implications are often negligible.
In summary, our result aligns closely with previous findings, with differences primarily in the constant
factors and the presence of G2. Despite these differences, the practical performance remains largely
unaffected, ensuring that the bound remains robust and applicable in a variety of scenarios.
F
Error Feedback applied to GaLore
F.1
Behaviour of the Error Feedback Mechanism
The GaLore low-rank updates introduced by [Zhao et al., 2024] enable the compression of optimizer
states by performing learning updates on a lower-dimensional subspace. In this approach, the
34

optimizer receives gradients projected on a defined learning subspace. Theoretical convergence
guarantees are provided under a “stable rank” assumption, where learning subspace is fixed during
training. However, in practice, convergence is attained by occasionally updating the learning subspace
and allowing full space learning to better align with the gradient trajectory during training.
Here, it is useful to draw an analogy with the TopK method, as the occasional updates of the learning
subspace resembles working with a fixed mask for many steps. Using a fixed mask would result
in discarding the same coordinates of the gradient at each step. Similarly, in the case of low-rank
updates, components orthogonal to the same learning subspace are discarded at each step.
The systematic nature of the information discarded by compression carries significant implications for
error feedback behavior. Over multiple steps, the error accumulates gradient components belonging
to the orthogonal space of the same learning subspace. Consequently, by linearity, the error itself
resides in the orthogonal space of this learning subspace. As a result, when the error is passed to the
accumulator, its projection onto the learning space is effectively disregarded until it is potentially
utilized at the specific step when the learning subspace is updated. Therefore, the behavior of error
feedback in the case of low-rank updates is non-standard: it accumulates gradient components over
numerous steps before unloading them all at once.
For a better understanding, we derive analytical evidence for the described behaviour by induction.
Let L be fixed learning subspace and assume that et−1 ∈L⊥. Then, gradient passed to the optimizer
at step t is: CGaLore(at) = projL(at) = projL(et−1 + gt) = projL(gt) where error feedback is
discarded. Thus, et = at −CGaLore(at) = et−1 + gt −projL(gt) = et−1 + projL⊥(gt) ∈L⊥
which completes the induction.
Assume now that learning subspace is updated every T steps, and denote Lt the learning subspace at
step t. Then, a similar induction leads to:
et =
t
X
i=1
t◦
j=iprojL⊥
j (gi) =
t
X
i=1
⌊t
T ⌋
◦
j=⌊i
T ⌋projL⊥
jT (gi)
akT = projLkT (gkT + ekT −1) = projLkT (gkT ) +
kT −1
X
i=1
projLkT ◦(
t◦
j=iprojL⊥
j )(gi)
F.2
Consequences on Training
Such behaviour of the error feedback mechanism results in the dominance of the error norm over the
gradient norm. Before learning subspace updates, the error is the sum over past gradient components
that belong to the orthogonal of the current learning subspaces. Since these components represent
descent directions that were not used, they are not expected to compensate each other on average.
Consequently, between learning subspace updates, the error norm is expected to grow linearly.
Figure 8 provides evidence of such linear growth of the error norm during fine-tuning of RoBERTa-
base model on GLUE/MNLI task.
It implies that known analysis techniques [Alistarh et al., 2018, Karimireddy et al., 2019] of con-
vergence for the error feedback mechanism do not apply to GaLore. Indeed, such proofs rely on
the assumption that the compression operator is contractive, as it allows the error to be bounded.
Given a fixed vector, low-rank compression based on its singular value decomposition is a contraction
operator. However, in our case, the compression is based on a previously-computed singular value
decomposition and therefore may not be a contraction operator for newly computed gradients. The
extreme case being when the gradient is orthogonal to the learning subspace, in which case the
compression operator returns the null vector. Figure 8 shows that during training the error norm is
not on the same order of magnitude of the gradient norm.
The dominance of the error over the gradient also has effects on space exploration, as the learning
subspaces are computed from the singular value decomposition of the accumulator (i.e. the sum of
the gradient and the error). Since the main components of the accumulator belong to the orthogonal
of current learning subspaces, successive learning subspaces will tend to be orthogonal to each other.
This allows errors to be effectively passed to the optimizer, but all at once which can introduce
irregularities in the learning trajectory. However, it also implies that learning is performed on a
learning subspace that is suboptimal in terms of the direction of the gradient, but this may help
35

Figure 8: Dynamics of the norm of the error compared to norm of the gradient (of output of the 3rd
attention layer) during fine-tuning of RoBERTa-base model on GLUE/MNLI from surrogate GaLore
with error feedback optimizer. We used hyperparameters from [Zhao et al., 2024], i.e. batch size 16,
learning rate 0.00001, projection update gap 200, rank 4 and GaLore scale 4.
convergence by enforcing space exploration. See Figure 9 for examples of how induced orthogonality
of successive learning subspaces affects the learning trajectory.
Figure 9: Optimization trajectory for Adam, GaLore-Adam and GaLore-Adam-EF for ill-conditioned
function f(x, y) = cos( 5π
4 x) + sin( 7π
4 y) starting from (x0, y0) = (−1
4, 1
4) (on first row) and for
Rosenbrock function starting from (x0, y0) = (−1
2, 1) (on second row).
36

NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction we introduce prior work on memory efficient
optimization and we claim that our work improves the memory usage while preserving the
performance. We provide theoretical and experimental justification for our algorithm that
reflect the claims in the abstract and introduction.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of our work in Section 6.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
37

Answer: [Yes]
Justification: We include brief theoretical justifications for our method in Section 4 and
include the complete proofs in the Appendix E.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We explain our experiments in Section 5 and provide the complete set of
hyper-parameters in Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
38

Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide a zip file that contains the code for our optimizer.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide information about the training details in Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We do not provide error bars, but instead explain how we report the results in
Appendix B. Concretely, we run the same experiment with three different seeds and report
the one with best performance.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
39

• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All these information can be found in Section 5.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Yes, the research conducted in this paper conforms, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss the broader impact of our work in Section 6.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
40

• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [No]
Justification: We do not release any models.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We work with open source models that are publicly available and we cited
them properly.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
41

• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide code and instructions on how to run the code in order to reproduce
our results.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects/
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
42

• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
43
Football is a family of team sports that involve, to varying degrees, kicking a ball to 
score a goal. Unqualified, the word football generally means the form of football that is 
the most popular where the word is used. Sports commonly 
called football include association football (known as soccer in Australia, Canada, South 
Africa, the United States, and sometimes in Ireland and New Zealand); Australian rules 
football; Gaelic football; gridiron football (specifically American football, arena football, 
or Canadian football); International rules football; rugby league football; and rugby union 
football.[1] These various forms of football share, to varying degrees, common origins 
and are known as "football codes". 
There are a number of references to traditional, ancient, or prehistoric ball games 
played in many different parts of the world.[2][3][4] Contemporary codes of football can be 
traced back to the codification of these games at English public schools during the 19th 
century, itself an outgrowth of medieval football.[5][6] The expansion and cultural power of 
the British Empire allowed these rules of football to spread to areas of British influence 
outside the directly controlled empire.[7] By the end of the 19th century, distinct regional 
codes were already developing: Gaelic football, for example, deliberately incorporated 
the rules of local traditional football games in order to maintain their heritage.[8] In 1888, 
the Football League was founded in England, becoming the first of many professional 
football associations. During the 20th century, several of the various kinds of football 
grew to become some of the most popular team sports in the world.[9]
AI is revolutionizing various fields by automating processes, improving decision-making, 
and boosting productivity. Machine learning and deep neural networks enable computers to process
massive data sets 
and identify trends. From voice recognition to self-driving cars, AI continues to be a major force of
progress 
in the modern era.
